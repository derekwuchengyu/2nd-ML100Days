{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Day080_HW.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwbvgCiaPtaE",
        "colab_type": "text"
      },
      "source": [
        "# 請結合前面的知識與程式碼，比較不同的 optimizer 與 learning rate 組合對訓練的結果與影響\n",
        "### 常見的 optimizer 包含\n",
        "* SGD\n",
        "* RMSprop\n",
        "* AdaGrad\n",
        "* Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUB2kVPMPtaF",
        "colab_type": "code",
        "outputId": "ee4a9972-67b4-44b4-f80d-c016c84a533e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "import keras\n",
        "\n",
        "# Disable GPU\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxhD02i0PtaI",
        "colab_type": "code",
        "outputId": "8d4c2863-8494-4ed3-fba8-9ef174487f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train, test = keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heTEEN_9PtaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 資料前處理\n",
        "def preproc_x(x, flatten=True):\n",
        "    x = x / 255.\n",
        "    if flatten:\n",
        "        x = x.reshape((len(x), -1))\n",
        "    return x\n",
        "\n",
        "def preproc_y(y, num_classes=10):\n",
        "    if y.shape[-1] == 1:\n",
        "        y = keras.utils.to_categorical(y, num_classes)\n",
        "    return y    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlG9s2Y4PtaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = train\n",
        "x_test, y_test = test\n",
        "\n",
        "# Preproc the inputs\n",
        "x_train = preproc_x(x_train)\n",
        "x_test = preproc_x(x_test)\n",
        "\n",
        "# Preprc the outputs\n",
        "y_train = preproc_y(y_train)\n",
        "y_test = preproc_y(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlir0aW8PtaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128, 128, 128]):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "    x = input_layer\n",
        "    \n",
        "    for i, n_units in enumerate(num_neurons):\n",
        "        x = keras.layers.Dense(units=n_units, activation=\"relu\", name=\"hidden_layer\"+str(i+1))(x)\n",
        "    \n",
        "    out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n",
        "    \n",
        "    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM5Ptps4PtaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 超參數設定\n",
        "\"\"\"\n",
        "Set your required experiment parameters\n",
        "\"\"\"\n",
        "LEARNING_RATE = [1e-2, 1e-3]\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 256\n",
        "MOMENTUM = 0.95\n",
        "OPT = ['SGD', 'RMSprop', 'AdaGrad', 'Adam']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7Pd3vMRdPtaU",
        "colab_type": "code",
        "outputId": "56d0bf62-a444-4ed8-9a5e-726f45ec644f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "results = {}\n",
        "\"\"\"Code Here\n",
        "撰寫你的訓練流程並將結果用 dictionary 紀錄\n",
        "\"\"\"\n",
        "for lr in LEARNING_RATE:\n",
        "  for index in range(4):\n",
        "    keras.backend.clear_session() # 把舊的 Graph 清掉\n",
        "    print(\"Experiment with LR = %.6f\" % (lr))\n",
        "    model = build_mlp(input_shape=x_train.shape[1:])\n",
        "    model.summary()\n",
        "    \n",
        "    if index == 0:\n",
        "      optimizer = keras.optimizers.SGD(lr=lr, nesterov=True, momentum=MOMENTUM)\n",
        "    elif index == 1:\n",
        "      optimizer = keras.optimizers.RMSprop(lr=lr)\n",
        "    elif index == 2:\n",
        "#       optimizer = keras.optimizers.AdaGrad(lr=lr)\n",
        "        continue\n",
        "    else:\n",
        "      optimizer = keras.optimizers.Adam(lr=lr)\n",
        "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n",
        "\n",
        "    model.fit(x_train, y_train, \n",
        "              epochs=EPOCHS, \n",
        "              batch_size=BATCH_SIZE, \n",
        "              validation_data=(x_test, y_test), \n",
        "              shuffle=True)\n",
        "    \n",
        "    # Collect results\n",
        "    train_loss = model.history.history[\"loss\"]\n",
        "    valid_loss = model.history.history[\"val_loss\"]\n",
        "    train_acc = model.history.history[\"acc\"]\n",
        "    valid_acc = model.history.history[\"val_acc\"]\n",
        "    \n",
        "    exp_name_tag = \"exp-lr-%s\" % str(lr)\n",
        "    results[exp_name_tag] = {'train-loss': train_loss,\n",
        "                             'valid-loss': valid_loss,\n",
        "                             'train-acc': train_acc,\n",
        "                             'valid-acc': valid_acc}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment with LR = 0.010000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 1.9000 - acc: 0.3095 - val_loss: 1.7502 - val_acc: 0.3770\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 1.6352 - acc: 0.4157 - val_loss: 1.6226 - val_acc: 0.4245\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 1.5398 - acc: 0.4516 - val_loss: 1.5304 - val_acc: 0.4480\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 1.4755 - acc: 0.4743 - val_loss: 1.4652 - val_acc: 0.4754\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 1.4220 - acc: 0.4909 - val_loss: 1.5318 - val_acc: 0.4586\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.3878 - acc: 0.5065 - val_loss: 1.4608 - val_acc: 0.4811\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 1.3501 - acc: 0.5172 - val_loss: 1.4386 - val_acc: 0.4901\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.3147 - acc: 0.5311 - val_loss: 1.4498 - val_acc: 0.4867\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.2900 - acc: 0.5383 - val_loss: 1.4190 - val_acc: 0.4995\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.2538 - acc: 0.5522 - val_loss: 1.3971 - val_acc: 0.4990\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.2338 - acc: 0.5605 - val_loss: 1.3562 - val_acc: 0.5233\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 11s 211us/step - loss: 1.2001 - acc: 0.5725 - val_loss: 1.3620 - val_acc: 0.5256\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 1.1720 - acc: 0.5819 - val_loss: 1.3562 - val_acc: 0.5248\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 1.1520 - acc: 0.5895 - val_loss: 1.3555 - val_acc: 0.5124\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 11s 211us/step - loss: 1.1277 - acc: 0.5980 - val_loss: 1.4173 - val_acc: 0.5086\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 11s 211us/step - loss: 1.1109 - acc: 0.6039 - val_loss: 1.3471 - val_acc: 0.5289\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.0795 - acc: 0.6143 - val_loss: 1.4398 - val_acc: 0.5059\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 1.0530 - acc: 0.6254 - val_loss: 1.3818 - val_acc: 0.5206\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 1.0309 - acc: 0.6306 - val_loss: 1.3623 - val_acc: 0.5276\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 1.0035 - acc: 0.6402 - val_loss: 1.3591 - val_acc: 0.5310\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 0.9750 - acc: 0.6485 - val_loss: 1.4160 - val_acc: 0.5207\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.9597 - acc: 0.6570 - val_loss: 1.4100 - val_acc: 0.5260\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 0.9329 - acc: 0.6664 - val_loss: 1.3800 - val_acc: 0.5303\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 0.9121 - acc: 0.6753 - val_loss: 1.4555 - val_acc: 0.5199\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 0.8900 - acc: 0.6808 - val_loss: 1.4383 - val_acc: 0.5316\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 11s 222us/step - loss: 0.8683 - acc: 0.6903 - val_loss: 1.5406 - val_acc: 0.5196\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 0.8545 - acc: 0.6932 - val_loss: 1.4564 - val_acc: 0.5296\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 11s 222us/step - loss: 0.8259 - acc: 0.7039 - val_loss: 1.4681 - val_acc: 0.5370\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 11s 222us/step - loss: 0.8036 - acc: 0.7128 - val_loss: 1.5278 - val_acc: 0.5234\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 0.7902 - acc: 0.7147 - val_loss: 1.5516 - val_acc: 0.5269\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 0.7621 - acc: 0.7260 - val_loss: 1.5342 - val_acc: 0.5317\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 11s 222us/step - loss: 0.7471 - acc: 0.7303 - val_loss: 1.5517 - val_acc: 0.5301\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 11s 222us/step - loss: 0.7188 - acc: 0.7409 - val_loss: 1.5929 - val_acc: 0.5233\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 0.7049 - acc: 0.7467 - val_loss: 1.6474 - val_acc: 0.5261\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 0.6881 - acc: 0.7544 - val_loss: 1.5783 - val_acc: 0.5247\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 0.6597 - acc: 0.7636 - val_loss: 1.6496 - val_acc: 0.5312\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 0.6421 - acc: 0.7667 - val_loss: 1.7204 - val_acc: 0.5257\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 0.6255 - acc: 0.7747 - val_loss: 1.7621 - val_acc: 0.5170\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 0.6128 - acc: 0.7794 - val_loss: 1.7289 - val_acc: 0.5291\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 0.5942 - acc: 0.7868 - val_loss: 1.7140 - val_acc: 0.5361\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 0.5848 - acc: 0.7873 - val_loss: 1.8427 - val_acc: 0.5251\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 0.5538 - acc: 0.7999 - val_loss: 1.8585 - val_acc: 0.5206\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 0.5498 - acc: 0.8003 - val_loss: 1.8888 - val_acc: 0.5211\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 0.5350 - acc: 0.8077 - val_loss: 2.1048 - val_acc: 0.5112\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 0.5253 - acc: 0.8106 - val_loss: 1.9170 - val_acc: 0.5415\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 0.5055 - acc: 0.8155 - val_loss: 1.9354 - val_acc: 0.5203\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 0.5034 - acc: 0.8211 - val_loss: 1.9577 - val_acc: 0.5248\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 0.4815 - acc: 0.8270 - val_loss: 2.0134 - val_acc: 0.5220\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 11s 222us/step - loss: 0.4683 - acc: 0.8291 - val_loss: 2.0631 - val_acc: 0.5287\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 0.4632 - acc: 0.8332 - val_loss: 2.0652 - val_acc: 0.5232\n",
            "Experiment with LR = 0.010000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 12s 240us/step - loss: 14.4454 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 12s 234us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 12s 233us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 12s 234us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 12s 234us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 12s 233us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 12s 236us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 12s 235us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 12s 235us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 12s 233us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 12s 235us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 12s 234us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 12s 236us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 12s 236us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 12s 230us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 12s 230us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 12s 230us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 11s 229us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 11s 229us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 11s 229us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 11s 229us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Experiment with LR = 0.010000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Experiment with LR = 0.010000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 14.4352 - acc: 0.0998 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 13s 254us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 13s 252us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 13s 252us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 13s 252us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 13s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 13s 252us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 13s 252us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 13s 252us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 13s 252us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 12s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 13s 252us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 13s 253us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 13s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 13s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 13s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 12s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 12s 248us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 12s 248us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 12s 245us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 12s 246us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 13s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 12s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 12s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Experiment with LR = 0.001000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 2.1199 - acc: 0.2318 - val_loss: 1.9487 - val_acc: 0.2969\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 1.8614 - acc: 0.3375 - val_loss: 1.8017 - val_acc: 0.3558\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 1.7487 - acc: 0.3794 - val_loss: 1.7059 - val_acc: 0.3977\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.6698 - acc: 0.4070 - val_loss: 1.6574 - val_acc: 0.4050\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.6135 - acc: 0.4246 - val_loss: 1.5972 - val_acc: 0.4271\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.5683 - acc: 0.4429 - val_loss: 1.5719 - val_acc: 0.4408\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.5316 - acc: 0.4546 - val_loss: 1.5386 - val_acc: 0.4490\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.4999 - acc: 0.4686 - val_loss: 1.5248 - val_acc: 0.4521\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.4668 - acc: 0.4774 - val_loss: 1.5077 - val_acc: 0.4685\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 1.4385 - acc: 0.4881 - val_loss: 1.4720 - val_acc: 0.4815\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 1.4137 - acc: 0.4963 - val_loss: 1.4935 - val_acc: 0.4667\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 1.3884 - acc: 0.5055 - val_loss: 1.4529 - val_acc: 0.4860\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.3638 - acc: 0.5135 - val_loss: 1.4218 - val_acc: 0.4912\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.3411 - acc: 0.5222 - val_loss: 1.5858 - val_acc: 0.4322\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 1.3208 - acc: 0.5307 - val_loss: 1.3992 - val_acc: 0.4999\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.3021 - acc: 0.5370 - val_loss: 1.4472 - val_acc: 0.4871\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 1.2809 - acc: 0.5448 - val_loss: 1.4261 - val_acc: 0.4884\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 1.2627 - acc: 0.5500 - val_loss: 1.4147 - val_acc: 0.5041\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 1.2430 - acc: 0.5573 - val_loss: 1.3735 - val_acc: 0.5141\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 1.2231 - acc: 0.5642 - val_loss: 1.3895 - val_acc: 0.5089\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 1.2112 - acc: 0.5681 - val_loss: 1.4336 - val_acc: 0.4960\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.1928 - acc: 0.5755 - val_loss: 1.3910 - val_acc: 0.5056\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 1.1722 - acc: 0.5815 - val_loss: 1.4511 - val_acc: 0.4952\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 1.1520 - acc: 0.5911 - val_loss: 1.3621 - val_acc: 0.5211\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.1404 - acc: 0.5916 - val_loss: 1.3961 - val_acc: 0.5076\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 1.1273 - acc: 0.5971 - val_loss: 1.3578 - val_acc: 0.5320\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.1073 - acc: 0.6027 - val_loss: 1.3760 - val_acc: 0.5203\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.0902 - acc: 0.6118 - val_loss: 1.4572 - val_acc: 0.4997\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 1.0767 - acc: 0.6159 - val_loss: 1.4027 - val_acc: 0.5142\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.0618 - acc: 0.6223 - val_loss: 1.4880 - val_acc: 0.5042\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 1.0494 - acc: 0.6264 - val_loss: 1.4473 - val_acc: 0.5076\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.0309 - acc: 0.6330 - val_loss: 1.4017 - val_acc: 0.5219\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 1.0150 - acc: 0.6385 - val_loss: 1.3861 - val_acc: 0.5271\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.0040 - acc: 0.6408 - val_loss: 1.4091 - val_acc: 0.5170\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 0.9859 - acc: 0.6495 - val_loss: 1.3672 - val_acc: 0.5314\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 0.9625 - acc: 0.6578 - val_loss: 1.3827 - val_acc: 0.5302\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 0.9551 - acc: 0.6584 - val_loss: 1.4134 - val_acc: 0.5267\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 0.9391 - acc: 0.6666 - val_loss: 1.4403 - val_acc: 0.5197\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 0.9231 - acc: 0.6718 - val_loss: 1.5422 - val_acc: 0.5007\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 0.9145 - acc: 0.6720 - val_loss: 1.3751 - val_acc: 0.5373\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 0.8945 - acc: 0.6803 - val_loss: 1.4875 - val_acc: 0.5154\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 0.8822 - acc: 0.6849 - val_loss: 1.4069 - val_acc: 0.5336\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 0.8649 - acc: 0.6928 - val_loss: 1.4786 - val_acc: 0.5208\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 0.8480 - acc: 0.6997 - val_loss: 1.4336 - val_acc: 0.5327\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 0.8320 - acc: 0.7058 - val_loss: 1.4490 - val_acc: 0.5297\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 0.8138 - acc: 0.7113 - val_loss: 1.5011 - val_acc: 0.5189\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 0.8023 - acc: 0.7130 - val_loss: 1.5206 - val_acc: 0.5229\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 0.7876 - acc: 0.7185 - val_loss: 1.8989 - val_acc: 0.4711\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 0.7792 - acc: 0.7222 - val_loss: 1.8728 - val_acc: 0.4562\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 0.7625 - acc: 0.7278 - val_loss: 1.5452 - val_acc: 0.5211\n",
            "Experiment with LR = 0.001000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 12s 236us/step - loss: 2.4965 - acc: 0.1854 - val_loss: 1.9929 - val_acc: 0.2618\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.9497 - acc: 0.2917 - val_loss: 1.9221 - val_acc: 0.2910\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.8446 - acc: 0.3342 - val_loss: 1.7999 - val_acc: 0.3464\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.7769 - acc: 0.3608 - val_loss: 1.8108 - val_acc: 0.3447\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.7278 - acc: 0.3797 - val_loss: 1.7858 - val_acc: 0.3389\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.6811 - acc: 0.4006 - val_loss: 2.2504 - val_acc: 0.2838\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.6529 - acc: 0.4086 - val_loss: 1.6389 - val_acc: 0.4186\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.6217 - acc: 0.4220 - val_loss: 1.7485 - val_acc: 0.3993\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 12s 234us/step - loss: 1.5946 - acc: 0.4302 - val_loss: 1.6837 - val_acc: 0.4063\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.5694 - acc: 0.4418 - val_loss: 1.6492 - val_acc: 0.4122\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.5510 - acc: 0.4435 - val_loss: 1.5771 - val_acc: 0.4303\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.5332 - acc: 0.4485 - val_loss: 1.7586 - val_acc: 0.4103\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.5158 - acc: 0.4565 - val_loss: 1.5856 - val_acc: 0.4244\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.5009 - acc: 0.4649 - val_loss: 1.6811 - val_acc: 0.4271\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4908 - acc: 0.4677 - val_loss: 1.6346 - val_acc: 0.4140\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4696 - acc: 0.4756 - val_loss: 1.6438 - val_acc: 0.4302\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.4675 - acc: 0.4772 - val_loss: 1.6213 - val_acc: 0.4380\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.4638 - acc: 0.4784 - val_loss: 1.5110 - val_acc: 0.4658\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 12s 233us/step - loss: 1.4619 - acc: 0.4776 - val_loss: 1.8061 - val_acc: 0.4004\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4634 - acc: 0.4816 - val_loss: 1.5546 - val_acc: 0.4532\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4429 - acc: 0.4876 - val_loss: 1.6225 - val_acc: 0.4449\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4514 - acc: 0.4851 - val_loss: 1.5284 - val_acc: 0.4675\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4624 - acc: 0.4823 - val_loss: 2.0046 - val_acc: 0.3700\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 12s 230us/step - loss: 1.4382 - acc: 0.4868 - val_loss: 1.8805 - val_acc: 0.4060\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4490 - acc: 0.4857 - val_loss: 1.5450 - val_acc: 0.4631\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.4536 - acc: 0.4841 - val_loss: 1.6996 - val_acc: 0.4115\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4548 - acc: 0.4850 - val_loss: 2.1967 - val_acc: 0.3669\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 12s 233us/step - loss: 1.4483 - acc: 0.4910 - val_loss: 1.7491 - val_acc: 0.4254\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.4433 - acc: 0.4921 - val_loss: 1.5748 - val_acc: 0.4482\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.4246 - acc: 0.4945 - val_loss: 1.8178 - val_acc: 0.3946\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 12s 230us/step - loss: 1.4447 - acc: 0.4912 - val_loss: 1.6854 - val_acc: 0.4129\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4256 - acc: 0.4962 - val_loss: 1.5851 - val_acc: 0.4459\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4509 - acc: 0.4903 - val_loss: 1.5710 - val_acc: 0.4659\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4551 - acc: 0.4862 - val_loss: 1.8294 - val_acc: 0.3573\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 12s 233us/step - loss: 1.4554 - acc: 0.4906 - val_loss: 2.2229 - val_acc: 0.4081\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 12s 235us/step - loss: 1.4354 - acc: 0.4991 - val_loss: 1.4758 - val_acc: 0.4845\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4551 - acc: 0.4923 - val_loss: 1.7047 - val_acc: 0.4420\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.4824 - acc: 0.4872 - val_loss: 1.9776 - val_acc: 0.3806\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.4650 - acc: 0.4925 - val_loss: 1.7510 - val_acc: 0.4262\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4654 - acc: 0.4906 - val_loss: 1.7663 - val_acc: 0.4177\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.4625 - acc: 0.4926 - val_loss: 1.7904 - val_acc: 0.3838\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.5100 - acc: 0.4801 - val_loss: 1.5228 - val_acc: 0.4823\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4796 - acc: 0.4877 - val_loss: 1.5716 - val_acc: 0.4636\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 12s 230us/step - loss: 1.4447 - acc: 0.4955 - val_loss: 1.4920 - val_acc: 0.4846\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.4752 - acc: 0.4906 - val_loss: 2.0217 - val_acc: 0.3809\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.5432 - acc: 0.4790 - val_loss: 1.5818 - val_acc: 0.4337\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.4922 - acc: 0.4926 - val_loss: 1.6459 - val_acc: 0.4402\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 12s 233us/step - loss: 1.5386 - acc: 0.4814 - val_loss: 2.6232 - val_acc: 0.3945\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.5874 - acc: 0.4812 - val_loss: 1.5592 - val_acc: 0.4607\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 1.4662 - acc: 0.4930 - val_loss: 3.2960 - val_acc: 0.2950\n",
            "Experiment with LR = 0.001000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Experiment with LR = 0.001000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 13s 259us/step - loss: 1.9087 - acc: 0.3054 - val_loss: 1.7468 - val_acc: 0.3799\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 12s 244us/step - loss: 1.7037 - acc: 0.3865 - val_loss: 1.6903 - val_acc: 0.3958\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 12s 245us/step - loss: 1.6231 - acc: 0.4178 - val_loss: 1.5700 - val_acc: 0.4425\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 12s 247us/step - loss: 1.5559 - acc: 0.4421 - val_loss: 1.5482 - val_acc: 0.4480\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 12s 246us/step - loss: 1.5058 - acc: 0.4604 - val_loss: 1.5425 - val_acc: 0.4504\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 12s 247us/step - loss: 1.4816 - acc: 0.4681 - val_loss: 1.4969 - val_acc: 0.4694\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 12s 248us/step - loss: 1.4324 - acc: 0.4895 - val_loss: 1.4681 - val_acc: 0.4843\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 1.4212 - acc: 0.4934 - val_loss: 1.4800 - val_acc: 0.4724\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 1.3796 - acc: 0.5065 - val_loss: 1.4170 - val_acc: 0.4947\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 1.3593 - acc: 0.5134 - val_loss: 1.4340 - val_acc: 0.4894\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 12s 247us/step - loss: 1.3471 - acc: 0.5186 - val_loss: 1.4416 - val_acc: 0.4812\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 13s 251us/step - loss: 1.3195 - acc: 0.5289 - val_loss: 1.4570 - val_acc: 0.4796\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 12s 247us/step - loss: 1.2838 - acc: 0.5411 - val_loss: 1.4293 - val_acc: 0.4946\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 12s 246us/step - loss: 1.2587 - acc: 0.5521 - val_loss: 1.4355 - val_acc: 0.4927\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 12s 247us/step - loss: 1.2441 - acc: 0.5549 - val_loss: 1.4120 - val_acc: 0.5025\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 12s 247us/step - loss: 1.2184 - acc: 0.5662 - val_loss: 1.4197 - val_acc: 0.5026\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 12s 248us/step - loss: 1.1983 - acc: 0.5702 - val_loss: 1.4079 - val_acc: 0.5082\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 12s 250us/step - loss: 1.1830 - acc: 0.5775 - val_loss: 1.3721 - val_acc: 0.5164\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 1.1518 - acc: 0.5871 - val_loss: 1.3731 - val_acc: 0.5210\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 1.1365 - acc: 0.5925 - val_loss: 1.3822 - val_acc: 0.5183\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 12s 247us/step - loss: 1.1045 - acc: 0.6050 - val_loss: 1.3763 - val_acc: 0.5208\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 12s 246us/step - loss: 1.0846 - acc: 0.6131 - val_loss: 1.4037 - val_acc: 0.5185\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 12s 247us/step - loss: 1.0696 - acc: 0.6157 - val_loss: 1.4045 - val_acc: 0.5210\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 12s 246us/step - loss: 1.0382 - acc: 0.6265 - val_loss: 1.4374 - val_acc: 0.5173\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 12s 246us/step - loss: 1.0201 - acc: 0.6328 - val_loss: 1.4104 - val_acc: 0.5217\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 1.0074 - acc: 0.6379 - val_loss: 1.4577 - val_acc: 0.5143\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 12s 246us/step - loss: 0.9849 - acc: 0.6467 - val_loss: 1.4214 - val_acc: 0.5245\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 0.9684 - acc: 0.6527 - val_loss: 1.4520 - val_acc: 0.5129\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 12s 246us/step - loss: 0.9401 - acc: 0.6608 - val_loss: 1.4838 - val_acc: 0.5161\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 12s 247us/step - loss: 0.9232 - acc: 0.6673 - val_loss: 1.4523 - val_acc: 0.5187\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 12s 247us/step - loss: 0.9071 - acc: 0.6750 - val_loss: 1.5669 - val_acc: 0.4976\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 12s 245us/step - loss: 0.8994 - acc: 0.6747 - val_loss: 1.5458 - val_acc: 0.5033\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 12s 244us/step - loss: 0.8607 - acc: 0.6898 - val_loss: 1.5560 - val_acc: 0.5132\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 12s 245us/step - loss: 0.8578 - acc: 0.6916 - val_loss: 1.5087 - val_acc: 0.5260\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 12s 246us/step - loss: 0.8330 - acc: 0.7010 - val_loss: 1.5599 - val_acc: 0.5150\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 12s 246us/step - loss: 0.8275 - acc: 0.7021 - val_loss: 1.5743 - val_acc: 0.5226\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 13s 250us/step - loss: 0.7955 - acc: 0.7109 - val_loss: 1.5929 - val_acc: 0.5241\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 12s 248us/step - loss: 0.7899 - acc: 0.7148 - val_loss: 1.5826 - val_acc: 0.5267\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 12s 247us/step - loss: 0.7565 - acc: 0.7261 - val_loss: 1.6638 - val_acc: 0.5250\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 12s 249us/step - loss: 0.7412 - acc: 0.7324 - val_loss: 1.6903 - val_acc: 0.5090\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 12s 247us/step - loss: 0.7325 - acc: 0.7344 - val_loss: 1.7216 - val_acc: 0.5026\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 12s 246us/step - loss: 0.7254 - acc: 0.7370 - val_loss: 1.7133 - val_acc: 0.5186\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 12s 247us/step - loss: 0.6977 - acc: 0.7457 - val_loss: 1.7575 - val_acc: 0.5173\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 12s 245us/step - loss: 0.6782 - acc: 0.7547 - val_loss: 1.8418 - val_acc: 0.5103\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 12s 245us/step - loss: 0.6764 - acc: 0.7554 - val_loss: 1.7889 - val_acc: 0.5119\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 12s 244us/step - loss: 0.6649 - acc: 0.7610 - val_loss: 1.8515 - val_acc: 0.5129\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 12s 244us/step - loss: 0.6487 - acc: 0.7649 - val_loss: 1.8758 - val_acc: 0.5066\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 12s 245us/step - loss: 0.6376 - acc: 0.7698 - val_loss: 1.8745 - val_acc: 0.5099\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 12s 247us/step - loss: 0.6204 - acc: 0.7751 - val_loss: 1.9380 - val_acc: 0.5079\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 12s 245us/step - loss: 0.6031 - acc: 0.7809 - val_loss: 1.9034 - val_acc: 0.5122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuBfxqSTPtaW",
        "colab_type": "code",
        "outputId": "8c9067d0-5bab-4519-cef5-7f95f221aced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\"\"\"Code Here\n",
        "將結果繪出\n",
        "\"\"\"\n",
        "color_bar = [\"r\", \"g\", \"b\", \"y\", \"m\", \"k\",\"o\",\"w\"]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "for i, cond in enumerate(results.keys()):\n",
        "    plt.plot(range(len(results[cond]['train-loss'])),results[cond]['train-loss'], '-', label=cond, color=color_bar[i])\n",
        "    plt.plot(range(len(results[cond]['valid-loss'])),results[cond]['valid-loss'], '--', label=cond, color=color_bar[i])\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "for i, cond in enumerate(results.keys()):\n",
        "    plt.plot(range(len(results[cond]['train-acc'])),results[cond]['train-acc'], '-', label=cond, color=color_bar[i])\n",
        "    plt.plot(range(len(results[cond]['valid-acc'])),results[cond]['valid-acc'], '--', label=cond, color=color_bar[i])\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAF1CAYAAAA5lJkfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuiD/vfoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAF1CAYAAADSoyIcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3XdcluX7//HXyRJRRAExRUUFHLgV\ncO9tjjTMWZqaaVZmWfqr7FN9yizLyjLN0tyhablFzZnjk+BMcQGCgqIyZK+b+/z9AfJ1pajAzTie\njwcPu6/rvK/r4CZ5e63zUFprhBBCCFF4mJm6ACGEEELcScJZCCGEKGQknIUQQohCRsJZCCGEKGQk\nnIUQQohCRsJZCCGEKGQknIUQQohCRsJZiEJGKbVHKRWrlCpl6lqEEKYh4SxEIaKUqgG0AzTQrwD3\na1FQ+xJCPJyEsxCFywvA/4DFwMhbC5VSpZVSXymlwpRScUqp/Uqp0tnr2iqlDiqlbiqlLiulRmUv\n36OUGnvbNkYppfbf9lorpSYqpS4AF7KXfZu9jXil1BGlVLvbxpsrpd5VSgUrpRKy11dTSs1VSn11\n+zehlNqglJqcHx+QECWBhLMQhcsLwIrsrx5KqUrZy78EmgOtAXvgHcColHIBtgLfARWBJsDxR9jf\nM0ALwCP7tX/2NuyBlcBvSinr7HVvAkOB3kA5YDSQDCwBhiqlzACUUo5A1+z3CyEeg4SzEIWEUqot\n4AKs1lofAYKBYdmhNxqYpLWO0Fpnaq0Paq3TgGHAn1rrX7XWGVrraK31o4TzZ1rrGK11CoDWenn2\nNgxa66+AUkCd7LFjgfe11ud0lhPZYw8DcUCX7HFDgD1a62tP+JEIUWJJOAtReIwEtmuto7Jfr8xe\n5ghYkxXWd6v2L8tz6/LtL5RSU5RSZ7JPnd8E7LL3/7B9LQFGZP/3CGDZE9QkRIknN4EIUQhkXz9+\nDjBXSkVmLy4FlAcqA6mAK3DirrdeBrz/ZbNJgM1tr5+6z5ictnTZ15ffIesI+LTW2qiUigXUbfty\nBU7dZzvLgVNKqcZAPWDdv9QkhMgFOXIWonB4Bsgk69pvk+yvesBfZF2HXgTMVkpVyb4xq1X2o1Yr\ngK5KqeeUUhZKKQelVJPsbR4HBiqlbJRSbsCYh9RgCxiAG4CFUuoDsq4t3/Iz8F+llLvK0kgp5QCg\ntQ4n63r1MmDtrdPkQojHI+EsROEwEvhFa31Jax156wv4HhgOTAP+ISsAY4DPATOt9SWybtB6K3v5\ncaBx9ja/BtKBa2Sddl7xkBq2AX7AeSCMrKP12097zwZWA9uBeGAhUPq29UuAhsgpbSGemNJaP3yU\nEEI8hFKqPVmnt120/GIR4onIkbMQ4okppSyBScDPEsxCPDkJZyHEE1FK1QNuknXj2jcmLkeIYkFO\nawshhBCFjBw5CyGEEIWMhLMQQghRyJhsEhJHR0ddo0YNU+1eCCGEKFBHjhyJ0lpXzM1Yk4VzjRo1\nCAgIMNXuhRBCiAKllArL7Vg5rS2EEEIUMrkKZ6VUT6XUOaVUkFJq2n3WV1dK7VZKHVNKnVRK9c77\nUoUQQoiS4aHhrJQyB+YCvcia93eoUsrjrmHvk9XmrilZ7eJ+yOtChRBCiJIiN9ecvYEgrXUIgFLK\nF+gPBN42RvN/E+TbAVcep5iMjAzCw8NJTU19nLeLJ2RtbU3VqlWxtLQ0dSlCCFGi5Sacnblz8vtw\noMVdYz4EtiulXgPKAF3vtyGl1DhgHED16tXvWR8eHo6trS01atRAKXXPepF/tNZER0cTHh5OzZo1\nTV2OEEKUaHl1Q9hQYLHWuipZHXKWKaXu2bbWeoHW2lNr7Vmx4r13k6empuLg4CDBbAJKKRwcHOSs\nhRBCFAK5CecIoNptr6tmL7vdGLJayaG1PgRYA46PU5AEs+nIZy+EEIVDbsLZH3BXStVUSlmRdcPX\nhrvGXAK6QM4k+NZkNWwXQGhoKA0aNMjV2LS0NAYPHoybmxstWrQgNDT0vuP8/PyoU6cObm5uzJw5\nM2f5999/j5ubG0opoqKi8qJ8IYQQBeyh4ay1NgCvktWI/QxZd2WfVkp9rJTqlz3sLeAlpdQJ4Fdg\nlLSNeziDwXDPsoULF1KhQgWCgoKYPHkyU6dOvWdMZmYmEydOZOvWrQQGBvLrr78SGJh1f16bNm34\n888/cXFxyff6hRBC5I9cXXPWWm/RWtfWWrtqrT/NXvaB1npD9n8Haq3baK0ba62baK2352fR+W35\n8uV4e3vTpEkTXn75ZcLCwnB3dycqKgqj0Ui7du3Yvn07oaGh1K1bl+HDh1OvXj18fHxITk5+4LYX\nL15Mv3796Ny5M126dLln/fr16xk5ciQAPj4+7Ny5k7v/nXP48GHc3NyoVasWVlZWDBkyhPXr1wPQ\ntGlTZFpUIYQo2kw2fedDvfEGHD+et9ts0gS+eXC72TNnzrBq1SoOHDiApaUlr7zyCnv37mXq1KlM\nmDABb29vPDw86N69O6GhoZw7d46FCxfSpk0bRo8ezQ8//MCUKVMeuI+jR49y8uRJ7O3t71kXERFB\ntWpZl/gtLCyws7MjOjoaR0fH+44BqFq1Kn///fejfBJCCCEKMZm+8y47d+7kyJEjeHl50aRJE3bu\n3ElISAhjx44lPj6e+fPn8+WXX+aMr1atGm3atAFgxIgR7N+//6H76Nat232DWQghROFwKe4SWy9s\nNdn+C++R80OOcPOL1pqRI0fy2Wef3bE8OTmZ8PBwABITE7G1tQXuvcNZKcXff//Nyy+/DMDHH39M\no0aN7hhTpkyZnP9+77332Lx5MwDHjx/H2dmZy5cvU7VqVQwGA3FxcTg4ONzx/ltjbgkPD8fZ2flJ\nvm0hhCjRjNpIwJUANp7byMbzGzlx7QRlrcoS9XYUpSxKFXg9cuR8ly5durBmzRquX78OQExMDGFh\nYUydOpXhw4fz8ccf89JLL+WMv3TpEocOHQJg5cqVtG3blhYtWnD8+HGOHz9Ov3797rufWz799NOc\nsQD9+vVjyZIlAKxZs4bOnTvf8w8ALy8vLly4wMWLF0lPT8fX1/eh+xFCCHGn5IxkNpzbwEsbXsJ5\ntjMtfm7BjP0zKFeqHF90/YKAlwJMEsxQmI+cTcTDw4NPPvmE7t27YzQasbS0ZPbs2fj7+3PgwAHM\nzc1Zu3Ytv/zyC506daJOnTrMnTuX0aNH4+HhwYQJE55o/2PGjOH555/Hzc0Ne3t7fH19Abhy5Qpj\nx45ly5YtWFhY8P3339OjRw8yMzMZPXo09evXB2DOnDl88cUXREZG0qhRI3r37s3PP//8xJ+LEEIU\nB0ExQfgF+eEX5MfOiztJNaRia2VLL/de9K3dl15uvXCwcXj4hvKZMtUTT56envrufs5nzpyhXr16\nJqnncYSGhtKnTx9OnTpl6lLyTFH7GQghxIMkpSexJ3QPW4O24hfkR3BsMABu9m70dutN3zp9ae/S\nHitzq3yvRSl1RGvtmZuxcuQshBCi2DBqIyciT7Dz4k62BW9jX9g+0jPTsbG0oXPNzkxuOZkebj1w\ns3czdakPJOH8BGrUqFGsjpqFEKKo0Vpz+sZpdl/cza7QXewN3UtsaiwA9SvW5zXv1+jp1pO21dti\nbWFt4mpzT8JZCCFEkWEwGjh9/TSHwg+xO3Q3e0L3cD0p6wbemuVrMqDuADrV7ESnGp1wLld0n2KR\ncBZCCFEoGbWR89Hn8Y/wJ+BKAP5X/DkWeYxUQ1b3PGdbZ3q49qBTjU50qtmJGuVrmLbgPCThLIQQ\nolAwaiP+Ef5svrCZfWH7OHr1KAnpCQCUsSxDs8rNmOA5Ac8qnng7e+NawbXYdtOTcBZCCGEy8Wnx\nbA/ezqbzm9gatJXrSdcxU2Y0r9ycFxq/gGcVT7yqeFHXsS7mZuamLrfASDgXgEd55CotLY0XXniB\nI0eO4ODgwKpVq+7byMLPz49JkyaRmZnJ2LFjmTZtGpDVMvKbb74hODiYGzdu3DEntxBCFDStNamG\nVFIMKSRnJJOckUxCWgL7wvax6cIm9oXtw2A0UMG6Aj3detKndh96uPYoFM8am5KEswkZDAYsLO78\nEdzeMtLX15epU6eyatWqO8bcahm5Y8cOqlatipeXF/369cPDw4M2bdrQp08fOnbsWIDfiRBCQHRy\nNN8f/p5lJ5dxM/UmKYYUUjJS0Nx/Po36FevzZss36VO7D62qtcLCTCLpFvkk7mP58uXMmTOH9PR0\nWrRowbvvvkvXrl05dOgQ9vb2dOjQgenTp1O7dm169uxJ8+bNOXr0KPXr12fp0qXY2Nj867YXL17M\n77//TmJiIpmZmezdu/eO9evXr+fDDz8EslpGvvrqq2it77iucnvLSCCnZaSHhwdNmzbN+w9ECCEe\nIDw+nNmHZrPgyAKSMpLo7tod1wqu2Fja5HyVtih9x+smTzWhZoWapi690Crc4Xy/o7/nnoNXXoHk\nZOjd+971o0ZlfUVFgY/Pnev27HnoLqVlpBBC5M756PN8ceALlp5YilEbGdpwKO+0foeGlRqaurQi\nr3CHswnc3jISICUlBScnJz788EN+++035s+fn9OkAu5tGTlnzpyHhrO0jBRCFGVHrhxh5oGZrA1c\nSymLUoxrPo63Wr0lR8J5qHCH84OOdG1sHrze0TFXR8p3k5aRQghxL601O0J2MOvgLP4M+ZNypcox\nre00JrWYRKWylUxdXrEjLSPvIi0jhRDi/2RkZrDsxDKa/NiEHst7cPr6aWZ2mcmlNy4xo8sMCeZ8\nIuF8l9tbRjZq1Ihu3boRGhqKv79/TkBbWVnxyy+/AOS0jKxXrx6xsbF50jIyOjoaNzc3Zs+ezcyZ\nM4GslpG9s6+x394ysl69ejz33HN3tIysWrUq4eHhNGrUiLFjxz5RPUKIkik+LZ6vDn5FrTm1eGHd\nC2QaM/ml/y9cnHSRqW2nYmdtZ+oSizVpGfkEpGWkEKI4Sc9MJzgmmMXHFzP/yHzi0+LpWKMjb7d+\nm15uvYrtbFwFRVpGCiGEuIfWmri0OEJiQwiOCSY4Njjnz5DYEC7HX8aojZgpMwZ5DGJK6yl4VslV\nlog8JuH8BKRlpBCiMDEYDZy8dpLLcZcJjw8nIiEi58+I+Kz/TspIuuM9FW0q4mrvStvqbXGt4Iqr\nvSvtXdoXqyYSRZGEsxBCFGEZmRnsuriL3wJ/Y93ZdUSnROesszCzoHLZylQtV5WGlRrS060nzrbO\n1KpQC1d7V2pVqEW5UuVMWL34NxLOQghRxKRnpvNnyJ/8Fvgb68+uJzY1FlsrW/rW6Uvf2n1xs3fD\n2dYZpzJOJapZRHEi4SyEEEXEwcsHmR8wnw3nNhCXFoddKTv61enHII9BdHPthrWFtalLFHlEwlkI\nIQq5qwlXeefPd1h+cjkVrCswsN5AfDx86FKzC6UsSpm6PJEP5DnnAhAaGkqDBg1yNTYtLY3Bgwfj\n5uZGixYtCA0Nve84Pz8/6tSpg5ubW86z0AAXL16kRYsWuLm5MXjwYNLT0wHYt28fzZo1w8LCgjVr\n1jzx9ySEyH8ZmRl8fehr6nxfh9WnV/Nu23e5PPkyi/ovord7bwnmYkzC2YQMBsM9y25vGTl58mSm\nTp16z5hbLSO3bt1KYGAgv/76K4GBgQBMnTqVyZMnExQURIUKFVi4cCEA1atXZ/HixQwbNix/vykh\nRJ7YF7aPZgua8eb2N2lTvQ2nJpzi0y6fUsaqzMPfLIo8Cef7WL58Od7e3jRp0oSXX36ZsLAw3N3d\niYqKwmg00q5dO7Zv305oaCh169Zl+PDh1KtXDx8fH5KTkx+47cWLF9OvXz86d+5Mly5d7lm/fv16\nRo4cCWS1jNy5cyd3TxRze8tIKyurnJaRWmt27dqFT3Y3rpEjR7Ju3Tog67GvRo0aYWYmP3IhCrMr\nCVcY/vtwOizuQEJaAn8M/oMtw7bg7uBu6tJEASq015zf8HuD45HHHz7wETR5qgnf9PzmgWOKcsvI\n6Ohoypcvj4WFRc7yiIiIXH8+QgjTiE+L5+jVo+wJ3cNXh74iIzOD6e2nM63tNGws/70/vCi+Cm04\nm4q0jBRC5KfkjGSORx4n4EoA/lf8CbgSwLmoc2iyzpA97f403/b8Fld7VxNXKkyp0Ibzw45w80tR\nbhnp4ODAzZs3MRgMWFhYSCtJIQqJ5IxkFh9fzM9Hf+bktZNk6kwAKpetjJezF8MaDMPL2QvPKp44\n2jg+ZGuiJCi04WwqXbp0oX///kyePBknJydiYmJISEjgyy+/ZPjw4bi4uPDSSy+xadMm4P9aRrZq\n1eqelpG3/Nsd15DVMvLTTz/NeX2rZWSrVq1y1TLS2dkZX19fVq5ciVKKTp06sWbNGoYMGcKSJUvo\n379/3n5AQohci0qOYu7huXzv/z1RyVF4VvFkWttpeFXxwsvZiyq2VUxdoiistNYP/QJ6AueAIGDa\nfdZ/DRzP/joP3HzYNps3b67vFhgYeM8yU/D19dWNGzfWDRs21M2aNdN79uzRLVq00AaDQWut9YAB\nA/SiRYv0xYsXdZ06dfTw4cN13bp19cCBA3VSUtI927t48aKuX7++1lrrX375RU+cOPFf952SkqJ9\nfHy0q6ur9vLy0sHBwVprrSMiInSvXr1yxm3evFm7u7vrWrVq6U8++SRneXBwsPby8tKurq7ax8dH\np6amaq21Pnz4sHZ2dtY2Njba3t5ee3h43Hf/heVnIERRFhwTrCdunqhLf1Ja8yG6z8o+em/oXm00\nGk1dmjAhIEDnInO11g9vGamUMs8O3G5AOOAPDNVaB/7L+NeAplrr0Q/arrSMLJyK2s9AiMLkyJUj\nzDo4i98Cf8NcmTO80XCmtJpCfaf6pi5NFAJ53TLSGwjSWodkb9wX6A/cN5yBocB/crNzIYQoCjIy\nM/A95cuxyGPEpcYRnx5PXGoccWlxd/yZYkihXKlyTGk1hddbvI5zObnnQzye3ISzM3D5ttfhQIv7\nDVRKuQA1gV1PXlrhJy0jhSjeMjIzWHpiKZ/+9SkXb17ExtKG8tblsStlh521HeWty+Ni55Lzukb5\nGjzf6HnsrO1MXboo4vL6hrAhwBqts29FvItSahwwDrJmrBJCiMIoPTOdJceXMGP/DEJvhuJZxZPv\nen1Hb/fe99ygKUR+yE04RwDVbntdNXvZ/QwBJv7bhrTWC4AFkHXNOZc1CiFEgUjPTGfx8cXM+GsG\nYXFheFXxYm7vufRy6yWhLApUbsLZH3BXStUkK5SHAPdM0KyUqgtUAA7laYVCCJEP0jPTuZ50nWuJ\n14hMjORc9Dm+/ftbLsVdooVzC+Y9PY+ebj0llIVJPDSctdYGpdSrwDbAHFiktT6tlPqYrNvCN2QP\nHQL46ofd/i2EEAXs1PVTfHnwS8Ljw4lMjCQyMZLolOh7xrWs2pIFfRbQ3bW7hLIwqVxdc9ZabwG2\n3LXsg7tef5h3ZRUvj/LIVVpaGi+88AJHjhzBwcGBVatWUaNGjXvG+fn5MWnSJDIzMxk7dizTpk0D\nslpGDhkyhOjoaJo3b86yZcuwsrJi3759vPHGG5w8eRJfX9+c5hhCFHerT6/mxfUvYmVuRT3HetR2\nqE17l/Y8VfapO74ql61M1XJVJZRFoSAzhJnQrWk2b3d7y0hfX1+mTp3KqlWr7hhzq2Xkjh07qFq1\nKl5eXvTr1w8PD4+clpFDhgxh/PjxLFy4kAkTJuS0jPzyyy8L8lsUwmQMRgP/78//x5eHvqRNtTb8\nNug3KttWNnVZQuSK9A+8D2kZKUTRdiPpBj2W9+DLQ18y0Wsiu0bukmAWRUqhPnLuuLjjPcueq/8c\nr3i9QnJGMr1X9L5n/agmoxjVZBRRyVH4rL7z1O2eUXseuk9pGSlE0XbkyhEGrh7ItcRrLO6/mJFN\nRpq6JCEemRxG3eX2lpFNmjRh586dhISEMHbsWOLj45k/f/4dp4bvbhm5f//+h+5DWkYKkT8WH19M\nm0VZfx8PjD4gwSyKrEJ95PygI10bS5sHrne0cczVkfLdtLSMFKLQuZZ4jeiUaMyVORZmFpibZf+Z\n/VopxX92/4cfAn6gc83O+D7rS8UyFU1dthCPrVCHsylIy0ghCodMYyZ+QX7MPzKfLRe2YNTGh75n\nSqspfNb1MyzM5FebKNrk/+C7eHh48Mknn9C9e3eMRiOWlpbMnj0bf39/Dhw4gLm5OWvXruWXX36h\nU6dO1KlTh7lz5zJ69Gg8PDyYMGHCE+1/zJgxPP/887i5uWFvb4+vry8AV65cYezYsWzZsgULCwu+\n//57evToQWZmJqNHj6Z+/ayuN59//jlDhgzh/fffp2nTpowZMwYAf39/BgwYQGxsLBs3buQ///kP\np0+ffrIPS4h8cDXhKguPLeSnoz9xKe4SlcpUYlqbaTR+qjGZxkwMRgOZOvvP7NcGo4H6TvXpWqur\nqcsXIk88tGVkfpGWkYVTUfsZiOLBqI3sDNnJ/CPzWX92PZk6k661ujK++Xj61emHpbmlqUsU4onl\ndctIIYTIc1prTl0/xerTq1l5aiUhsSE4lHbgzVZvMq75ONzs3UxdohAmI+H8BKRlpBCP7syNM6w+\nvZpVp1dxJuoMZsqMTjU68d9O/+XZes9SyqKUqUsUwuQknIUQ+e5C9AVWnV7F6tOr+ef6PygU7V3a\n85r3awysN5BKZSuZukQhCpVCF85aa5nb1kSkZ4nISxmZGaw7u445h+ew/1LW8/9tqrVhTs85POvx\nLFVsq5i4QiEKr0IVztbW1kRHR+Pg4CABXcC01kRHR2NtbW3qUkQRdyPpBj8d/Ykf/H8gIiGCmuVr\n8kXXLxjSYAjV7Ko9fANCiMIVzlWrViU8PJwbN26YupQSydramqpVq5q6DFFEHbt6jDmH5/DrP7+S\nlplG11pdmff0PHq798bczNzU5QlRpBSqcLa0tKRmzZqmLkMIkUuxKbFsOr+JBUcXsP/SfmwsbRjd\ndDSver+KR0UPU5cnRJFVqMJZCFH4XY67zPpz61l3dh17QveQqTOpVaEWs7vP5sWmL1LeurypSxSi\nyJNwFkI8kNaawBuBrDu7jnXn1hFwJWvyoLqOdXmnzTs8U/cZPKt4Yqakj44QeUXCWQhxX6mGVJYc\nX8I3f3/D2aizALSs2pKZXWbSv25/6jrWNXGFQhRfEs5CiDvcTL3JPP95fPv3t1xLuoZnFU/mPT2P\nfnX6yeNPQhQQCWchBAAR8RF8879v+PHIjySkJ9DDtQdT20ylY42O8mijEAVMwlmIEu7MjTPMOjiL\n5SeXk6kzGVx/MO+0eYcmTzUxdWlClFgSzkKUMMkZyfwV9hc7QnawI2QHJ6+dxNrCmnHNx/FWq7eo\nWUEeZxTC1CSchSjmMo2ZHL16lD9D/mRHyA4OXD5AemY6VuZWtKnWhpldZjK66Wgqlqlo6lKFENkk\nnIUohlIyUtgWvI01gWvYGrSVmJQYABpXasxr3q/RrVY32rm0w8bSxsSVCiHuR8JZiGIiKT2JrUFb\nWRO4hk3nN5GUkYR9aXv61u5Ld9fudKnZRbo/CVFESDgLUYQlpiey+fxm1pxZw5YLW0jOSKaiTUVG\nNBqBj4cPHVw6YGluaeoyhRCPSMJZiCIoIzODH/x/4KO9HxGbGkulMpUY1XgUPh4+tHNph4WZ/NUW\noiiTv8FCFCFaazad38SUHVM4H32ebrW68V6792hbva10fhKiGJFwFqKIOHntJG9ue5OdF3dSx6EO\nm4Zuord7b5kgRIhiSMJZiELuWuI1pu+ezsJjCylvXZ45Pecw3nO8XEsWohiTcBaikDFqI1cSrhAU\nE8RfYX8x6+AsUgwpvO79OtM7TMe+tL2pSxRC5DMJZyFMJCUjhYOXDxIUE5T1FRvEhegLBMcGk2pI\nzRnXt3Zfvuz+JbUdapuwWiFEQZJwFqKAhd0MY17APH4++jPRKdEAWFtY41rBFTd7N3q69cTN3g03\nezdqO9Smul11E1cshChoEs5CFACtNbsu7uJ7/+/ZcG4DAM/UfYYxTcfQqFIjqthWwUyZmbhKIURh\nIeEsRD5KTE9k2YllfO//PYE3AnG0cWRqm6mM9xwvR8RCiH+Vq3BWSvUEvgXMgZ+11jPvM+Y54ENA\nAye01sPysE4hipRUQyqf7vuUOYfnEJ8WT/PKzVncfzGDGwzG2sLa1OUJIQq5h4azUsocmAt0A8IB\nf6XUBq114G1j3IH/B7TRWscqpZzyq2AhCruDlw8yZsMYzkadZZDHICa3nEzLqi3leWQhRK7l5sjZ\nGwjSWocAKKV8gf5A4G1jXgLmaq1jAbTW1/O6UCEKu6T0JN7f9T7f/v0t1e2qs23ENrq7djd1WUKI\nIig34ewMXL7tdTjQ4q4xtQGUUgfIOvX9odba7+4NKaXGAeMAqleX622i+Nh1cRcvbXyJkNgQJnpN\n5LMun2FbytbUZQkhiqi8uiHMAnAHOgJVgX1KqYZa65u3D9JaLwAWAHh6euo82rcQJhOXGsc7O95h\nwdEFuNm7sXfUXtq7tDd1WUKIIi434RwBVLvtddXsZbcLB/7WWmcAF5VS58kKa/88qVKIQiY9M53N\n5zfzut/rXEm4wpRWU/io00fYWNqYujQhRDGQm3D2B9yVUjXJCuUhwN13Yq8DhgK/KKUcyTrNHZKX\nhQphamE3w/AL8mNr0FZ2XtxJYnoi9SvWZ+1za/F29jZ1eUKIYuSh4ay1NiilXgW2kXU9eZHW+rRS\n6mMgQGu9IXtdd6VUIJAJvK21js7PwoXIb6mGVP4K+4utQVvxC/LjTNQZAKrbVWd4w+H0dOtJL7de\nlLIoZeJKhRDFjdLaNJd+PT09dUBAgEn2LcSDRCZG8vn+z/np6E8kZSRRyrwUHWp0oKdrT3q69aSu\nY115LEoI8ciUUke01p65GSszhAmR7VriNb448AXzAuaRnpnO8EbDGVx/MB1rdJRryUKIAiXhLEq8\nG0k3mHVwFnP955JqSGVEoxFMbz8dN3s3U5cmhCihJJxFiRWVHMWXB7/k+8Pfk2JIYVjDYUxvP11a\nMwohTE7CWZQ4/1z7h4XHFrLw2EKS0pMY2nAo09tPp65jXVOXJoQQgISzKCHiUuPwPeXLwmML8b/i\nj5W5FT4ePrzX7j08KnqYujwfRs9SAAAgAElEQVQhhLiDhLMotrTW/HXpLxYeW8hvp38jxZBCQ6eG\nfNPjG0Y0GoGDjYOpSxRCiPuScBbFSnRyNIcjDnMo/BC+p3y5EHMBWytbnm/0PGObjcWziqc8BiWE\nKPQknEWRlZKRwrHIYxyOOJzzFRwbDIBC0bZ6W95r9x4+Hj6UsSpj4mqFECL3JJxFkeMX5MeHez7k\nyNUjGIwGAKqVq4a3szfjmo/D29mb5pWbS1coIUSRJeEsioyLsReZvG0y68+tx93enXdav0OLqi3w\nquJFZdvKpi5PCCHyjISzKPSSM5L5fP/nfH7gcyzMLPi86+e80fINrMytTF2aEELkCwlnUWhprVl3\ndh2Tt00mLC6MoQ2GMqvbLJzLOZu6NCGEyFcSzqJQOht1lte3vs6OkB00cGrAnpF76FCjg6nLEkKI\nAiHhLEwqJiWGC9EXuBBzgQvRFwiKDeJC9AWORR6jjGUZ5vScwwSvCViYyf+qQoiSQ37jiQKVlJ7E\nrIOz8Avy40LMBWJSYnLWKRTV7arj7uDO5JaTmdJ6Ck5lnExYrRBCmIaEsygQWmt8T/ny9o63iUiI\noF31dgzyGIS7vTvuDu6427tTs0JNrC2sTV2qEEKYnISzyHdHrx7l9a2vc+DyAZpVbsYqn1W0qd7G\n1GUJIUShJeEs8s31pOu8t/M9Fh5biKONIz/1/YkXm7yIuZm5qUsTQohCTcJZ5LmMzAzm+s/lwz0f\nkpSRxBst3+CDDh9Q3rq8qUsTQogiQcJZ5KkDlw4wbtM4Am8E0sO1B1/3+Jp6FeuZuiwhhChSJJxF\nnohPi2fan9OYFzAPFzsX1g9ZT9/afaUDlBBCPAYJZ/HE1p9dz8QtE7maeJU3WrzBfzv/l7JWZU1d\nlhBCFFkSzuKxRSZG8trW11gTuIaGTg35ffDveDt7m7osIYQo8iScxSPTWrPo2CKm7JhCSkYKn3b+\nlLdbv42luaWpSxNCiGJBwlk8kpPXTjLJbxJ7QvfQwaUDC/ouoLZDbVOXJYQQxYqEs8iVS3GX+GD3\nByw9sRQ7azsW9FnAmGZjMFNmpi5NiCemtebg5YM0rNSQcqXKFdh+M42ZnLp+iqCYIIJjg7kYexFL\nc0t6ufWil3svjNqIf4Q/lcpWolKZSpS2LF1gtQnTknAWDxSTEsNnf33Gd4e/A2BK6ylMazsN+9L2\nJq5MFITz0edZcGQBEzwn4Grvaupy8o3BaGDC5glcS7rGZ10+Y1STUU/0D89MY2bOZDt7Q/dyNfEq\nUclRXE+6TkhsCM0rN2dyq8kYtZHmC5qTqTMBsC9tT6Yxk/LW5enl3ouo5ChaLmyZs11bK1s61ujI\nL/1/wcHG4cm+aVGoSTiL+0rJSOG7w9/x2f7PiEuNY2STkXzU8SOq21U3dWkFJiI+AqcyTo99Ld1g\nNKC1fuz3p2emE5kYSRXbKo/VlSsyMZIDlw5gMBroW6cvNpY2uX7vlYQrfLTnIxYeW4hTGSdmdp2Z\nU5OVudUj11IYaa1Ze2Yt3Wp1w87ajjm95vDuzncZs2EMP/j/wLc9v83VNLNaawKuBPDjkR/ZdXEX\nUclRtHNpx+ZhmwF4/o/nuRx/GQAzZUa1ctWoVq4aAJbmlqwbso4qtlVwreCKnbVdzjYhK4w3Dt3I\ntcRrXEu6Rnh8OIuOLWL/pf30r9s/Pz4WUUioW/8TFDRPT08dEBBgkn2Lf2cwGlh2Yhkf7PmA8Phw\nerv3ZmaXmTSs1NDUpeWrjMwMjl49SnBsMMMaDgOg5c8tCbgSgEt5F9zt3XGzd8Orihcjm4wEso6O\nYlJiuHjzIiGxIYTEhuBu786g+oNIyUih3MxyGLWRKrZVcLFzwaW8Cz71fBhQbwBGbeRc1DnKlSpH\nWFwYF2OzttHbvTfNqzRnX9g+Oi7uiEZjbWFNQ6eGNKvcjNdbvI5HRY976jdqIyGxIbjZuwEw4vcR\nrPhnRc76cqXKMdFrIjO6zHjg55CYnsiMv2bwzf++wWA0MN5zPO+3fx+nMk7EpMTQ7MdmvNTsJaa0\nnkIpi1J59fEXuOtJ13ll8yusPbOWTzp9wnvt3wOyQvHXU7/yzo53iEiIYP+L+x8a0MN/H87Kf1Zi\nY2lDb/feONs607hSY15s+iIAR64cobRlaRxtHLEvbf/E7U8j4iNwLuec833kdee2jMwMEtMTsbO2\nk8tWeUwpdURr7ZmbsXLkLLgUd4ltQdvYHrKdP0P+5GbqTbydvVk2YBkda3Q0dXn55kTkCTac28C+\nS/s4ePkgyRnJWFtY4+Phg5W5FTO7zmRnyM6cHtOHwg9xLvpcTjh7/ODB+ejzd2xzZOORDKo/iNKW\npfmo40ekGlIJiwsj7GYYhy4fopFTIyDrqNbjh3tD1tHGkeZVmlPboTYfdPiAymUrcyHmAkevHsX3\nlC+jm44GYE3gGj7Z9wlNnmrCjeQbHLx8kIS0BG5Ou0lZq7L0qd2Hpk81pW31tqQYUlh0bBEGowHI\nCqDFxxfzTN1nqFC6wh37N1NmLD2xlAH1BvDfTv+lVoVaOesyMjPwcvbi/d3vs+zkMub2nkuXWl1y\n9VmnZKRwNuoswbHBBMcEZ/0ZG8ysbrNoVrkZ+y/t59jVY7zW4rVc/vQe35rANUzYPIH4tHg+7/o5\nb7Z6M2edUophDYfRv05/lp9cTutqrYGsme+aVm5KaYvS+F/xZ9GxRXzR7QvKlSrHwLoDaVe9HcMa\nDrvv9ermVZrnaf23gvl/4f+jy9IuzO4+m5c9X86TbZ+IPEE/335ciruEQtG/bn/+GPwHACPXjSTN\nkIZ9aXtqlq/J4AaDS9SZtIImR84lUFJ6EntC97A9eDvbQ7ZzNuosAM62zvRw7UH/uv2L7exekYmR\nOJR2wNLckhl/zeD9Xe/T+KnGtK/envYu7WlbvS2Vyla673u11qQYUnJOD3/212fYWNpQq0Italao\nSc3yNSljVSZXdSSkJbDx/EbiUuNwKe9CrQq1cLFzeeANP1prNBozZYZfkB/f/v0txyOPU8G6Am2r\nt6VNtTb4ePg8tAb/CH+8f/amlHkpnvV4Fs/Knmw8vxG/EX5YmVsRnxb/wJui/IL8eHXLqwTHBjO0\nwVB+6f9LzlF0XGocp2+c5vT105y+cRofDx/aVm/LvrB9dFjcIWcbFW0q4mrvyqxus2hbvS2j1o1i\nyYklLH1mKc83fj5Xn+HjmHVgFu/8+Q6eVTxZ3H8x9Z3qP/Q9calxVPu6GhVKV8C+tD3HI49TxrIM\nm4dtpkONDg99f36JT4tn6NqhbLmwhUktJvFV96+eqKnMtqBtPLv6Wcpbl2dSi0kkpCdQo3yNnH8Q\n9ljeg9CbocSkxBCVHAXAe+3e45POn+TJ91MSPMqRs4RzCZGYnsgfZ/5gxT8r2B26m/TMdEpblKZD\njQ50r9WdHm49qOdYr1gGMkDAlQC+/ftbVp1axdIBSxnSYAgxKTGYKbMS2ZDj6NWjLDy6kBX/rCAu\nLQ6vKl6sfW4t1eyq5er9KRkpfH7gcwJvBLJ60GquJlzF+2dvwuPDc8bYWNrwdY+vGdd8HHGpcfwZ\n8ieu9q7UqlDrnvBPM6Tx9Mqn2Ru2l41DN9LTrWeefa8Go4GYlBicyjhxKe4SK06u4O02bz/S6eW9\noXuZtnMaGZkZjG029l+PkgtapjGTKdun8M3f39DLrRe+Pr6PXdetxyRXDFxBFdsqDxx7MfYiy08u\nx9vZmx5uPQi7GcZ7u97jhcYv0KVml3/9R8KtS0HXk66TlJGUM2nRzpCd/HP9HxLSEkhITyA+LR6n\nMk5Mbz/dJPMnaK3z5XehhLMAsn4p7QzZybKTy/jj7B8kZyRTo3wNfOr50MOtB22rt8XawtrUZeYb\nozay+vRq5vw9h0Phh7C1suXFJi8yqeWkO07XlmQpGSmcjz5Po0qNHuuX0a1fYpnGTMZuHEtdh7rU\nd6qPR0UPapSv8UjXLOPT4um4uCPnos+xe+TuJ55t7tbP/4PdH+Bm78aW4VueaHuF2Y8BPzJxy0S+\n6v4Vk1pOyvX70jPTWRu4lqENhwKPH0pbLmxh+O/DuZl6kyq2VRhYdyAazZxeczBTZvxn93+YFzCP\n6JRojNoIgJW5FWnvpwFZp8yXnlgKQGmL0tiWsqVSmUqcnHASgNWnV+Nu706Tp5o8tD6jNhJ6M5TK\nZStT2rI0aYY0lFIPvJHxZupN/g7/m4OXD3Iw/CDXk65zYvyJR/4cHkbCuQTTWnMs8hjLTizj11O/\nci3pGuWtyzO4/mBGNBpBm2ptiu3RMWT9xbwYexFXe1e01tT/oT4Zxgxe836NUU1GFYqjHfHvIhMj\nab2wNT1cezCvz7zH2obWmo3nNzJ993ROXjtJA6cGfNLpk2J/d/PxyOM0qtQIM2XGqlOreKrsU7St\n3vZfj2Kjk6N5dvWz7A3by//G/I8WVVs80f5TDalsOr+JpSeW4hfkR1mrsgS/HkyF0hVY+c9K9oXt\no6JNRZzKOFGxTEXKW5enh2sPlFLEpsQCYFvKNueMxq0nAwxGA5W+rERMSgyuFVwZ5DEIHw8fmlVu\nhlKKiPgIfgv8jX+u/cOpG6c4ff00SRlJ7B65m441OrLi5ApG/DGCijYVcS7njLNt1tf0DtOpWq4q\nH+75kI/3fpxzyaihU0NaV2vNtz2/zfOj9jwPZ6VUT+BbwBz4WWs98671o4BZQET2ou+11j8/aJsS\nznnvwKUDvLLlFU5eO4mlmSV9avdhRKMRPO3+dJG+s/ZhEtIS2BGyg83nN7MlaAsJaQlEvROFtYU1\nl+Mu41zOWe46LUIiEyNxKuP02D+zef7zeGXLK7jZu/Fxx48Z3GBwifr5a61xnePKxZsXearsU/jU\n8+G5+s/RpnqbnM/hbNRZ+qzsw+X4yyzst5ARjUbkeQ15eRBwI+kG686u47fA39h1cReZOjPn/oTD\nEYdp8XMLnMo40cCpAQ2dGtLAqQFPuz9NZdvKnLx2kj/O/EFEQkTWV3wEVxKucPilw9QoX4OdITs5\nePkgrau1xtvZG9tStnlW993yNJyVUubAeaAbEA74A0O11oG3jRkFeGqtX81tkRLOeSfNkMZ/9vyH\nWQdnUd2uOlPbTOW5+s8V2YlCtNZkGDOyTnsZ0riedB2NRmuNURvRaJzKOFHWqiyLji1i/KbxZBgz\nsCtlR0+3njzt/jSD6g8q1qfsS4LLcZeZsmMKC/osyHn+935SMlLYeXEndqXsaOfSjrjUONYEruGF\nxi+U2Pnek9KT2HxhM6tPr2bzhc2kGlKZ3HIys3vMZmfITnx+88HSLOsZ61t3pBcV0cnRrDu7jkpl\nK9Gndh8yMjOITY3N80fK8kNeP0rlDQRprUOyN+4L9AcCH/guUSBORJ7g+T+e55/r/zC26Vhm95id\nr//yy2+7L+7m/d3vM7bpWF5s+iJHrx6l9aJ7f3n8Nug3fDx8aF65OW+0fIOn3Z+mdbXWJfaXcXF0\nLvocv5/5netJ1/Eb7nfH2Z+rCVfZdH4TG89v5M+QP0kxpNC5Zmd2vrATO2s7xjQbY8LKTa+MVRme\nq/8cz9V/jsT0RDad30QdhzoAxKXFUd2uOuuHrKdG+RqmLfQxONg43PHztTS3LBLB/KhyE87OwOXb\nXocD97s48axSqj1ZR9mTtdaX7zNG5BGD0cCsA7P4z57/YF/ank1DN/F07adNXdZjO3j5INN3T2fX\nxV042zpTxzHrF4mrvSs/9f0JhcJMmaGUQqHwrJL1j8/GTzWm8VONTVm6yCdda3Vlcf/FjPhjBM//\n8TyfdP4kp8nKkLVD2Be2Dxc7F8Y0HUPfOn3p4GK6x5oKs7JWZRnSYEjO64H1BtKvTr8nngxF5K/c\nnNb2AXpqrcdmv34eaHH7KWyllAOQqLVOU0q9DAzWWne+z7bGAeMAqlev3jwsLCzvvpMS5EL0BUau\nG8mh8EP4ePgw7+l5ONo4mrqsxzZu4zh+OvoTTmWceLftu7zs+bKckhY5vjr4FVN2TMHCzIIbb9+g\nvHV5/g7/mzJWZahfsX6xvsFRFC95fVo7Arj94ceq/N+NXwBoraNve/kz8MX9NqS1XgAsgKxrzrkp\nUPyfjMwMfjr6E2/veBsrcytWDFzB0AZD7/nlFBIbkjXVYuVmBX4jTGJ6Ip/s+4SEtAR6uvWkb52+\naK05ee0klW0r42jjiJky49T1U7jbu1PKohQdXDrgWsGVV71fzfUkHqLkeKv1W9iXtkcphaVZ1mWL\nJ72zWIjCLjfh7A+4K6VqkhXKQ4Bhtw9QSlXWWl/NftkPOJOnVZZARm3kQvQF/K/44x/hz+Erhzke\neZxUQyrdXbuzqN+inGn8ICu4b6bepGKZiswPmM+sg7NwtHGkW61u9HTrSXfX7jxV9ql8rfl89HkG\nrBrA2aizlLcuj521HX3r9OVm6k2a/NgEAAszCyqVqcSVhCt81+s7JnpPZHij4flalyj6bs1TLURJ\n8dBw1loblFKvAtvIepRqkdb6tFLqYyBAa70BeF0p1Q8wADHAqHysudg6ff00y08ux/+KPwFXAohL\niwOyZlpqVrkZEzwn0N6lPf3r9L/jaFlrzcQtE9kevJ0T408wpfUUGldqjF+wH9uDt/PrqV+pYF2B\nG2/fwNzMnNiU2HvmVH5S24O3M+i3QViaWbJ9xHa61OqS01nH2sKaNYPWcCXhClcTr3Il4Qoudi45\nEx8IIYS4k0xCUkicvn6atr+0JTE9kUaVGuFVxQuvKl54O3tTr2K9B968ceua3Ltt3+XTLp/esc6o\njZyIPEHozdCcbkj15tbD0caRSS0mMaDugDy5w/nU9VO8uuVVljyzBJfyLk+8PSGEKG5khrAiJjw+\nnFYLW5GRmcGO53c8UnvGdWfXMXDVQHw8fPD18X3oNeb0zHTm+c/ju8PfERwbjLOtM694vcK45uMe\n+aaymJQYVv6zkle9s+4NzK/5aIUQojh4lHAuOdPmFFI3U2/Sa0Uv4lLjGNZwGO1+acePAT/mzD/7\nIMeuHmPY2mF4O3uz5Jklubr5y8rcikktJ3Hu1XNsHLqRehXr8d6u99gTugcgV/uFrKkCPRd48ua2\nN3O6WkkwCyFE3pAH3UwozZDGgFUDOHPjDH4j/Gjh3IJ9YfsYv3k8S08u5cc+P9LAqcG/vt+lvAs+\nHj7M6jbrga0G78fczJw+tfvQp3YfAm8E5jw/OuOvGaw+vRrncs5Zc+DaVOSpsk8xpfUUAMJuhrHr\n4i4mbplIhdIV2PfiPuo61n38D0EIIcQ95LS2iRi1kcG/DWbNmTWUK1WOsxPPUtm2Mlprlp1cxpvb\n3iQuLY6ZXWbyVuu37nhvYnoilmaW+TJf9sKjC1l7Zi03km9wPek615OuU9aqLDfevgHAs6uf5fcz\nv9PepT2rfVb/a+9jIYQQd5JrzkXAuA3j+OnYTwC80/odPu3y6R03fUUlR/HOjnfoW7svA+oNyLme\nazAa6PdrP1INqfz5wp/5/hyz1poUQwo2ljZA1kxe1xKv0ad2H5kqUwghHkFeT0Ii8tiETRP46dhP\nWJpZsvLZlfh4+NwzxtHGkUX9F+W8nvHXDE7fOE0ZyzJsDdrK/KfnF8gEI0qpnGAGitwk+UIIURRJ\nOBcw31O+zD8yH1srWw6NOUR9p/q5ep+5mTlrz6wlPTOdt1q9xcueL+dzpUIIIUxFTmsXkDRDGn5B\nfjy35jmaV27O+iHrqVim4iNt41zUOfaG7WVM0zH/2kBdCCFE4SSntQsRozayLWgbYzaM4WriVeo5\n1mPzsM2PNUNXHcc6Od2ahBBCFF8SzvnkRtINFh9fzI9HfiQ4NhiAzjU7s2LgijyfOlMIIUTxIuGc\nh7TW7A3by49HfuT3M7+TnpmOfWl7AOb0nMNrLV4zcYVCCCGKAgnnPHI88jjD1g7jTNQZyluXZ3zz\n8UQlR7Hy1ErmPT2P8Z7jTV2iEEKIIkLCOQ8cjjhMj+U9KGtVlsX9FzOo/iBsLG2IiI+gt3tvaYko\nhBDikcjc2k9o/6X9dF3aFfvS9vz14l8MaTCEBUcWkGnMxLmcswSzEEKIRybh/AR2XdxFj+U9qGJb\nhX2j9lG5bGWeXf0sk7dNZnfoblOXJ4QQooiScH5MWy9spfeK3tSqUIsVA1dwPPI4A1YNYPOFzcx/\nej5da3U1dYlCCCGKKLnmnAtaa45HHud/4f8j9GYo+y/v59DlQzRwasCukbv4+tDXzNg/AzNlxvyn\n58vsXUIIIZ6IhPO/iEuNw9LcEhtLG37w/4FXt74KgIWZBQajAbtSdvz67K842jgyuulo+tbpi2sF\n10ee9UsIIYS4m4RzNq01gTcC2XJhC1uCtrD/0n6WPLOEYQ2H0bdOX8palSU6OZopO6bQ3qU9m4Zu\nwraULQCu9q642rua+DsQQghRXEg4Z3t29bP8cfYPABpVasSUVlNo8lQTAKrbVcdMmfHWjrfoWqsr\n64esv6NTkxBCCJGXJJyB9Mx0bEvZ8mbLN5ncajJVy1W9Y33AlQBe2vgSnWt2ZuPQjVhbWJuoUiGE\nECWBhDNgZW7FkmeW3HddVHIUz65+lkplK7HKZ5UEsxBCiHxX4h+lSkxP5NjVY/ddl2nMZNjaYUQm\nRrL2ubU42jgWcHVCCCFKohIfziv/WUmzBc04Hnn8nnUf7P6AHSE7mNt7Lp5VctWCUwghhHhiJTqc\ntdbMD5hPo0qNaFyp8R3r1p9dz4z9MxjbdCxjm401UYVCCCFKohIdzocjDnMs8hjjm49HKZWz/Hz0\neV5Y9wKeVTz5rvd3JqxQCCFESVSiw3n+kfmUsSxzR3OKxPREBq4aiKWZJWsGrZEbwIQQQhS4Enu3\ndnpmOlsvbGVEoxGUK1UOyDrNPXbDWM5EnWHbiG24lHcxcZVCCCFKohIbzlbmVgS9HkRyRnLOsm//\n/pZVp1fxWZfPpHGFEEIIkymR4ay1BqCsVVnKWpUFYF/YPqZsn8KAugOY2maqKcsTQghRwpXIa857\nw/bScF5DzkadBSAhLYFha4fhau/K4mcW33FzmBBCCFHQSuSR87yAeUQkROBil3VN+eO9HxOREMHB\n0Qdzrj8LIYQQplLijpyvJV7j9zO/M6rxKEpblubU9VN8/b+vGdt0LK2qtTJ1eUIIIUTJC+dFxxZh\nMBoY7zkerTWvbH6F8tblmdl1pqlLE0IIIYASdlo705jJgqML6FSjE3Uc67D0xFL+uvQXP/f9GQcb\nB1OXJ4QQQgC5PHJWSvVUSp1TSgUppaY9YNyzSimtlCqUE1FrNDM6z2B6++nEpsQyZfsUWlVtxYtN\nXzR1aUIIIUSOhx45K6XMgblANyAc8FdKbdBaB941zhaYBPydH4XmBQszC4Y2HArAK5tfITolmu1P\nb8dMlbiz+0IIIQqx3KSSNxCktQ7RWqcDvkD/+4z7L/A5kJqH9eWZy3GXmbl/JrEpsQRcCWB+wHxe\n836NJk81MXVpQgghxB1yE87OwOXbXodnL8uhlGoGVNNab37QhpRS45RSAUqpgBs3bjxysU/ip6M/\n8e7Od4lJiWHC5glUKluJjzp+VKA1CCGEELnxxOdzlVJmwGzgrYeN1Vov0Fp7aq09K1as+KS7zrXY\nlFh+Pvozvdx7sT14OwFXApjdfTZ21nYFVoMQQgiRW7kJ5wig2m2vq2Yvu8UWaADsUUqFAi2BDYXl\nprAL0RdoubAlUclRvNTsJd7d9S6da3ZmSIMhpi5NCCGEuK/chLM/4K6UqqmUsgKGABturdRax2mt\nHbXWNbTWNYD/Af201gH5UvEj0Fozav0oYlJi2PnCTtadXUdSehJze8+VKTqFEEIUWg+9W1trbVBK\nvQpsA8yBRVrr00qpj4EArfWGB2/BNDKNmZibmbP0maUopYiIj2DJiSX8v7b/j7qOdU1dnhBCCPGv\n1K0OTQXN09NTBwTk/cG1wWjgrW1vEZUSxfIBy1FKkWnMpMmPTUhISyBwYiA2ljZ5vl8hhBDiQZRS\nR7TWubrkW6we8I1LjaPPyj7MOTyHSmUqYdRGAI5ePcqp66f4qONHEsxCCCEKvWIzfWdwTDB9f+3L\nhZgL/NT3J8Y2G5uzbv+l/QB0c+1mqvKEEEKIXCsW4WwwGui5oicxKTHseH4HHWt0vGP9/sv7qVWh\nFlVsq5imQCGEEOIRFItwtjCzYGG/hVSxrYKbvdsd67TW7L+0n55uPU1UnRBCCPFoikU4A7R3aX/f\n5UExQVxPuk7bam0LuCIhhBDi8RSrG8Lu59b15rbVJZyFEEIUDcU+nP+69BcOpR3k2WYhhBBFRrEP\n5/2X9tOmehuZEUwIIUSRUazD+VriNS7EXJDrzUIIIYqUYh3OBy4fAOR6sxBCiKKlWIfz/kv7sbaw\nplnlZqYuRQghhMi1Yh/O3s7elLIoZepShBBCiFwrtuGclJ7E0atH5XqzEEKIIqfYhvPfEX////bu\nLkau+rzj+Pdhl8XYBhubxYB3jSl1QhbxpiyEl6hNEU1ME0Eu0gpEFCpFQpWCRJVWLX0RVZFy0URK\n2wsuglrUXJQSmjapU7kCBCjUIVBvYmpiu04MIdjG2EuMAQN+WfvpxRnLw84Szy6zzH/Ofj/SaM85\n/7Mzj585Z3478x/PcCSPON8sSeo5tQ3ndS+tIwiuHr6626VIkjQttQ7ni5ddzOJ5i7tdiiRJ01LL\ncJ44OsEPd/zQ+WZJUk+qZThv3L2R/Yf2O98sSepJtQxnv+xCktTLahvOKxatYHjRcLdLkSRp2moX\nzpnJupfW+axZktSzahfOP9/3c3bt3+WbwSRJPat24ex8sySp19UynBedsoiLzrqo26VIkjQjtQzn\na1dcy0lRu3+aJGmOqFWCvfr2q2x5dYvzzZKknlarcH5q+1OA882SpN5Wq3Be99I6BvoGuGL5Fd0u\nRZKkGatdOI+eO8q8/nndLkWSpBmrTTi/c/gdxl4ec75ZktTzahPO619ez+Gjh51vliT1vNqE87EP\nH7lm+JouVyJJ0vtTq+pa4rMAAAsdSURBVHAeGRxh6fyl3S5FkqT3pRbhfOToEZ7a/pTzzZKkWmgr\nnCNidURsjYhtEXHXFON/EBHPRcSzEbEuIkY6X+p72zS+idcPvu58sySpFk4YzhHRB9wL3ACMALdM\nEb4PZObFmXkZ8FXg6x2v9Ffwyy4kSXXS38Y+VwLbMvMFgIh4ELgJ2Hxsh8x8o2n/BUB2ssgT+cKl\nX+DCMy9k5eKVH+TNSpI0K9oJ5+XA9qb1HcDHJu8UEV8CvgwMANd1pLo2LRxYyHXnf6A3KUnSrOnY\nG8Iy897MvAD4U+Avp9onIm6PiLGIGBsfH+/UTUuSVCvthPNOYLhpfaix7b08CHx2qoHMvC8zRzNz\ndHBwsP0qJUmaQ9oJ5/XAqog4PyIGgJuBNc07RMSqptVPAz/rXImSJM0tJ5xzzsyJiLgDeBjoA+7P\nzE0RcQ8wlplrgDsi4nrgMPAacNtsFi1JUp2184YwMnMtsHbStrublu/scF2SJM1ZtfiEMEmS6sRw\nliSpMIazJEmFMZwlSSqM4SxJUmEMZ0mSCmM4S5JUGMNZkqTCGM6SJBXGcJYkqTCGsyRJhTGcJUkq\njOEsSVJhDGdJkgpjOEuSVBjDWZKkwhjOkiQVxnCWJKkwhrMkSYUxnCVJKozhLElSYQxnSZIKYzhL\nklQYw1mSpMIYzpIkFcZwliSpMIazJEmFMZwlSSqM4SxJUmEMZ0mSCmM4S5JUGMNZkqTCGM6SJBXG\ncJYkqTCGsyRJhWkrnCNidURsjYhtEXHXFONfjojNEbExIh6LiPM6X6okSXPDCcM5IvqAe4EbgBHg\nlogYmbTbBmA0My8Bvg18tdOFSpI0V7TzzPlKYFtmvpCZh4AHgZuad8jMJzLz7cbq08BQZ8uUJGnu\naCeclwPbm9Z3NLa9ly8C/zXVQETcHhFjETE2Pj7efpWSJM0hHX1DWER8HhgFvjbVeGbel5mjmTk6\nODjYyZuWJKk2+tvYZycw3LQ+1Nj2LhFxPfAXwG9m5sHOlCdJ0tzTzjPn9cCqiDg/IgaAm4E1zTtE\nxOXAN4AbM3NP58uUJGnuOGE4Z+YEcAfwMLAFeCgzN0XEPRFxY2O3rwELgX+NiGcjYs17XJ0kSTqB\ndl7WJjPXAmsnbbu7afn6DtclSdKc5SeESZJUGMNZkqTCGM6SJBXGcJYkqTCGsyRJhTGcJUkqjOEs\nSVJhDGdJkgpjOEuSVBjDWZKkwhjOkiQVxnCWJKkwhrMkSYUxnCVJKozhLElSYQxnSZIKYzhLklQY\nw1mSpMIYzpIkFcZwliSpMIazJEmFMZwlSSqM4SxJUmEMZ0mSCmM4S5JUGMNZkqTCGM6SJBXGcJYk\nqTCGsyRJhTGcJUkqjOEsSVJhDGdJkgpjOEuSVBjDWZKkwrQVzhGxOiK2RsS2iLhrivHfiIgfR8RE\nRHyu82VKkjR3nDCcI6IPuBe4ARgBbomIkUm7vQT8PvBApwuUJGmu6W9jnyuBbZn5AkBEPAjcBGw+\ntkNmvtgYOzoLNUqSNKe0E87Lge1N6zuAj81OOTP0ve/BrbdWy5nVBeC002DRIjh8GF5+ufX3Tjut\nuhw5Art3t46ffjosXAgTE7BnT+v4okWwYAEcOgSvvto6vngxzJ8PBw/CL3/ZOr5kCcybBwcOwN69\nreNLl8Ipp8Dbb8O+fa3jZ54JAwPw1lvw+uut42edBf39sH8/vPFG6/iyZdDXB2++WV0mO/tsOOmk\n6rrfeqt1/Nxzq5/79lU1NouAc86pll97Dd55593jJ51UXT9U//YDB9493t9f1Q9Vbw8devf4ySfD\n4GC1PD5e3cfNBgaq/kB1301MvHt83ryq/wCvvAJHJ/1deeqpcMYZ1fKuXcePqWPmz6/uX5j62Fqw\noDo+jh6trn8yjz2PPfDYK/3YGx6GDRta9/kAtBPOHRMRtwO3A6xYsaJzVzwwUN0ZEcdu6PgJumpV\ndXDv39/6exdcUF0OHIAnn2wd/9CHYOXK6iD4wQ9axz/yERgaqu7EZ55pHb/ooqqGvXthbKx1/OKL\nq5N8fHzqA+DSS6sDedcueO651vHLL69OlB07YPPm1vGPfrQ6iV58EX7609bxK66oTpLnn68uk111\nVdXXrVvhF79oHb/mmurn5s1VDc36+4+Pb9zY+iBxyinHxzdsqHrQbP784+NjY60n8emnV/UBPP10\n60m4ZAmMjlbL69a1PoAPDlb9A/j+96sHkmZnnw2XXFItP/546wPs0BCMNGZ3HnmEFuedBx/+cPV7\njz/eOu6x57EHHnulH3vH7qcuaCecdwLDTetDjW3Tlpn3AfcBjI6O5gl2b9+nPjX1X2CSJPWgdt6t\nvR5YFRHnR8QAcDOwZnbLkiRp7jphOGfmBHAH8DCwBXgoMzdFxD0RcSNARFwRETuA3wW+ERGbZrNo\nSZLqrK0558xcC6ydtO3upuX1VC93S5Kk98lPCJMkqTCGsyRJhTGcJUkqjOEsSVJhDGdJkgpjOEuS\nVBjDWZKkwhjOkiQVxnCWJKkwhrMkSYWJnPxdoR/UDUeMA1N8F9yMnQlM8eWimgF72Tn2snPsZefY\ny86Ybh/Py8zBdnbsWjh3WkSMZeZot+uoA3vZOfayc+xl59jLzpjNPvqytiRJhTGcJUkqTJ3C+b5u\nF1Aj9rJz7GXn2MvOsZedMWt9rM2csyRJdVGnZ86SJNVCLcI5IlZHxNaI2BYRd3W7nl4SEfdHxJ6I\n+EnTtiUR8WhE/Kzx84xu1tgLImI4Ip6IiM0RsSki7mxst5fTFBHzIuJ/IuJ/G73868b28yPimcZ5\n/q2IGOh2rb0iIvoiYkNE/Gdj3V7OQES8GBHPRcSzETHW2DYr53jPh3NE9AH3AjcAI8AtETHS3ap6\nyj8Bqydtuwt4LDNXAY811vWrTQB/lJkjwFXAlxrHob2cvoPAdZl5KXAZsDoirgL+BvjbzPx14DXg\ni12ssdfcCWxpWreXM/dbmXlZ03+hmpVzvOfDGbgS2JaZL2TmIeBB4KYu19QzMvNJYO+kzTcB32ws\nfxP47AdaVA/KzF2Z+ePG8ptUD4TLsZfTlpX9jdWTG5cErgO+3dhuL9sUEUPAp4F/aKwH9rKTZuUc\nr0M4Lwe2N63vaGzTzC3LzF2N5VeAZd0sptdExErgcuAZ7OWMNF6GfRbYAzwKPA/sy8yJxi6e5+37\nO+BPgKON9aXYy5lK4JGI+FFE3N7YNivneH8nrkT1lZkZEb6lv00RsRD4N+APM/ON6klKxV62LzOP\nAJdFxGLgO8CFXS6pJ0XEZ4A9mfmjiPhEt+upgY9n5s6IOAt4NCL+r3mwk+d4HZ457wSGm9aHGts0\nc7sj4hyAxs89Xa6nJ0TEyVTB/M+Z+e+NzfbyfcjMfcATwNXA4og49oTC87w91wI3RsSLVFN+1wF/\nj72ckczc2fi5h+qPxiuZpXO8DuG8HljVePfhAHAzsKbLNfW6NcBtjeXbgP/oYi09oTGP94/Alsz8\netOQvZymiBhsPGMmIk4FfptqDv8J4HON3exlGzLzzzJzKDNXUj02Pp6Zt2Ivpy0iFkTEaceWgU8C\nP2GWzvFafAhJRPwO1bxKH3B/Zn6lyyX1jIj4F+ATVN+ushv4K+C7wEPACqpvDvu9zJz8pjE1iYiP\nA/8NPMfxub0/p5p3tpfTEBGXUL2xpo/qCcRDmXlPRPwa1bO/JcAG4POZebB7lfaWxsvaf5yZn7GX\n09fo2Xcaq/3AA5n5lYhYyiyc47UIZ0mS6qQOL2tLklQrhrMkSYUxnCVJKozhLElSYQxnSZIKYzhL\nklQYw1mSpMIYzpIkFeb/AcWv2oPN92geAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}