{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Day079_HW.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwgoWkteIn8P",
        "colab_type": "text"
      },
      "source": [
        "## Work\n",
        "1. 請比較 SGD optimizer 不同的 momentum 及使用 nesterov 與否的表現"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vITLL8oXIn8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import keras\n",
        "\n",
        "# 本作業可以不需使用 GPU, 將 GPU 設定為 \"無\" (若想使用可自行開啟)\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdFj_AFmIn8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EidCVZIcIn8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 資料前處理\n",
        "def preproc_x(x, flatten=True):\n",
        "    x = 2*(((x - x.min()) / (x.max() - x.min()))-0.5)\n",
        "    if flatten:\n",
        "        x = x.reshape((len(x), -1))\n",
        "    return x\n",
        "\n",
        "def preproc_y(y, num_classes=10):\n",
        "    if y.shape[-1] == 1:\n",
        "        y = keras.utils.to_categorical(y, num_classes)\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn6WfhNvIn8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = train\n",
        "x_test, y_test = test\n",
        "\n",
        "# 資料前處理 - X 標準化\n",
        "x_train = preproc_x(x_train)\n",
        "x_test = preproc_x(x_test)\n",
        "\n",
        "# 資料前處理 -Y 轉成 onehot\n",
        "y_train = preproc_y(y_train)\n",
        "y_test = preproc_y(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mn3dFT5In8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128, 128, 128]):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "    \n",
        "    for i, n_units in enumerate(num_neurons):\n",
        "        if i == 0:\n",
        "            x = keras.layers.Dense(units=n_units, activation=\"relu\", name=\"hidden_layer\"+str(i+1))(input_layer)\n",
        "        else:\n",
        "            x = keras.layers.Dense(units=n_units, activation=\"relu\", name=\"hidden_layer\"+str(i+1))(x)\n",
        "    \n",
        "    out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n",
        "    \n",
        "    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky1j8lPoIn8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Code Here\n",
        "設定超參數\n",
        "\"\"\"\n",
        "LEARNING_RATE = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 256\n",
        "MOMENTUM = [1,0.95,0.75,0.5,0.25]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MwRWxEK9In8d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5f7535f-6ee5-44cc-b1f9-ed1836127876"
      },
      "source": [
        "results = {}\n",
        "\"\"\"Code Here\n",
        "撰寫你的訓練流程並將結果用 dictionary 紀錄\n",
        "\"\"\"\n",
        "for lr, mom in zip(LEARNING_RATE, MOMENTUM):\n",
        "  for i in range(2):\n",
        "    keras.backend.clear_session() # 把舊的 Graph 清掉\n",
        "    print(\"Experiment with LR = %.6f\" % (lr))\n",
        "    model = build_mlp(input_shape=x_train.shape[1:])\n",
        "    model.summary()\n",
        "    if i == 0:\n",
        "      nesterov = True\n",
        "    else:\n",
        "      nesterov = False\n",
        "    optimizer = keras.optimizers.SGD(lr=lr, nesterov=nesterov, momentum=mom)\n",
        "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n",
        "\n",
        "    model.fit(x_train, y_train, \n",
        "              epochs=EPOCHS, \n",
        "              batch_size=BATCH_SIZE, \n",
        "              validation_data=(x_test, y_test), \n",
        "              shuffle=True)\n",
        "    \n",
        "    # Collect results\n",
        "    train_loss = model.history.history[\"loss\"]\n",
        "    valid_loss = model.history.history[\"val_loss\"]\n",
        "    train_acc = model.history.history[\"acc\"]\n",
        "    valid_acc = model.history.history[\"val_acc\"]\n",
        "    \n",
        "    exp_name_tag = \"exp-lr-%s\" % str(lr)\n",
        "    results[exp_name_tag] = {'train-loss': train_loss,\n",
        "                             'valid-loss': valid_loss,\n",
        "                             'train-acc': train_acc,\n",
        "                             'valid-acc': valid_acc}"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment with LR = 0.100000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 12s 242us/step - loss: 13.8181 - acc: 0.1048 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 11s 229us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 12s 230us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Experiment with LR = 0.100000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 3.6937 - acc: 0.1691 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 14.5076 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 11s 222us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Experiment with LR = 0.010000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 11s 230us/step - loss: 1.7528 - acc: 0.3763 - val_loss: 1.5327 - val_acc: 0.4616\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 1.4249 - acc: 0.4954 - val_loss: 1.4122 - val_acc: 0.5014\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 1.2929 - acc: 0.5435 - val_loss: 1.3926 - val_acc: 0.5089\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 1.1915 - acc: 0.5801 - val_loss: 1.3906 - val_acc: 0.5119\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 1.1079 - acc: 0.6053 - val_loss: 1.3599 - val_acc: 0.5264\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 1.0290 - acc: 0.6372 - val_loss: 1.3507 - val_acc: 0.5347\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 0.9547 - acc: 0.6602 - val_loss: 1.3839 - val_acc: 0.5354\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 0.8809 - acc: 0.6888 - val_loss: 1.4074 - val_acc: 0.5328\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 0.8188 - acc: 0.7092 - val_loss: 1.4955 - val_acc: 0.5246\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 11s 229us/step - loss: 0.7564 - acc: 0.7324 - val_loss: 1.5643 - val_acc: 0.5242\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 0.7027 - acc: 0.7497 - val_loss: 1.5706 - val_acc: 0.5287\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 0.6500 - acc: 0.7669 - val_loss: 1.6018 - val_acc: 0.5304\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 0.5913 - acc: 0.7912 - val_loss: 1.7334 - val_acc: 0.5195\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 11s 229us/step - loss: 0.5558 - acc: 0.8033 - val_loss: 1.7197 - val_acc: 0.5224\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 11s 229us/step - loss: 0.5144 - acc: 0.8159 - val_loss: 1.8085 - val_acc: 0.5191\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 0.4718 - acc: 0.8330 - val_loss: 2.0035 - val_acc: 0.5210\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 0.4407 - acc: 0.8438 - val_loss: 2.0186 - val_acc: 0.5242\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 0.4020 - acc: 0.8582 - val_loss: 2.1322 - val_acc: 0.5214\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 0.3780 - acc: 0.8664 - val_loss: 2.1727 - val_acc: 0.5283\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 0.3562 - acc: 0.8742 - val_loss: 2.2249 - val_acc: 0.5297\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 0.3342 - acc: 0.8827 - val_loss: 2.2571 - val_acc: 0.5231\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 0.3045 - acc: 0.8919 - val_loss: 2.2786 - val_acc: 0.5267\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 11s 230us/step - loss: 0.2911 - acc: 0.8971 - val_loss: 2.4479 - val_acc: 0.5208\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 0.2731 - acc: 0.9044 - val_loss: 2.5354 - val_acc: 0.5286\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 0.2511 - acc: 0.9122 - val_loss: 2.5297 - val_acc: 0.5243\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 0.2352 - acc: 0.9178 - val_loss: 2.6477 - val_acc: 0.5236\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 0.2358 - acc: 0.9164 - val_loss: 2.6528 - val_acc: 0.5317\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 0.2164 - acc: 0.9235 - val_loss: 2.7140 - val_acc: 0.5174\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 0.1991 - acc: 0.9313 - val_loss: 2.8634 - val_acc: 0.5231\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 0.1838 - acc: 0.9366 - val_loss: 2.8999 - val_acc: 0.5276\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 0.1901 - acc: 0.9339 - val_loss: 2.9423 - val_acc: 0.5232\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 0.1783 - acc: 0.9381 - val_loss: 2.9699 - val_acc: 0.5253\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 0.1687 - acc: 0.9406 - val_loss: 2.9622 - val_acc: 0.5262\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 0.1701 - acc: 0.9407 - val_loss: 2.9715 - val_acc: 0.5193\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 0.1611 - acc: 0.9443 - val_loss: 3.1523 - val_acc: 0.5214\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 0.1487 - acc: 0.9492 - val_loss: 3.1353 - val_acc: 0.5292\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 11s 230us/step - loss: 0.1496 - acc: 0.9483 - val_loss: 3.1308 - val_acc: 0.5332\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 0.1454 - acc: 0.9504 - val_loss: 3.1077 - val_acc: 0.5242\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 0.1313 - acc: 0.9546 - val_loss: 3.1575 - val_acc: 0.5256\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 0.1271 - acc: 0.9562 - val_loss: 3.2588 - val_acc: 0.5314\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 0.1138 - acc: 0.9599 - val_loss: 3.3738 - val_acc: 0.5357\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 0.1078 - acc: 0.9641 - val_loss: 3.3085 - val_acc: 0.5221\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 0.1102 - acc: 0.9629 - val_loss: 3.3530 - val_acc: 0.5301\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 0.1033 - acc: 0.9650 - val_loss: 3.4623 - val_acc: 0.5321\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 0.1076 - acc: 0.9639 - val_loss: 3.3814 - val_acc: 0.5260\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 0.1044 - acc: 0.9654 - val_loss: 3.3688 - val_acc: 0.5323\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 0.1011 - acc: 0.9661 - val_loss: 3.3213 - val_acc: 0.5285\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 0.0890 - acc: 0.9698 - val_loss: 3.3061 - val_acc: 0.5289\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 0.0932 - acc: 0.9680 - val_loss: 3.3086 - val_acc: 0.5349\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 0.0938 - acc: 0.9680 - val_loss: 3.4747 - val_acc: 0.5372\n",
            "Experiment with LR = 0.010000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 1.7905 - acc: 0.3595 - val_loss: 1.5928 - val_acc: 0.4307\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 1.4791 - acc: 0.4746 - val_loss: 1.4495 - val_acc: 0.4826\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 1.3586 - acc: 0.5194 - val_loss: 1.3994 - val_acc: 0.5046\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 1.2551 - acc: 0.5575 - val_loss: 1.3893 - val_acc: 0.5108\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 1.1633 - acc: 0.5885 - val_loss: 1.3733 - val_acc: 0.5200\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 1.0980 - acc: 0.6126 - val_loss: 1.3409 - val_acc: 0.5353\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.0225 - acc: 0.6385 - val_loss: 1.3906 - val_acc: 0.5228\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 0.9625 - acc: 0.6590 - val_loss: 1.3821 - val_acc: 0.5224\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 0.9023 - acc: 0.6815 - val_loss: 1.3835 - val_acc: 0.5335\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 0.8391 - acc: 0.7000 - val_loss: 1.4630 - val_acc: 0.5346\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.7763 - acc: 0.7263 - val_loss: 1.5159 - val_acc: 0.5290\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 0.7356 - acc: 0.7388 - val_loss: 1.5701 - val_acc: 0.5346\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 0.6805 - acc: 0.7567 - val_loss: 1.5844 - val_acc: 0.5261\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.6340 - acc: 0.7734 - val_loss: 1.6153 - val_acc: 0.5274\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 0.5983 - acc: 0.7861 - val_loss: 1.6886 - val_acc: 0.5214\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 0.5714 - acc: 0.7973 - val_loss: 1.7512 - val_acc: 0.5219\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.5041 - acc: 0.8205 - val_loss: 1.8193 - val_acc: 0.5207\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 0.4813 - acc: 0.8307 - val_loss: 1.8880 - val_acc: 0.5341\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.4484 - acc: 0.8416 - val_loss: 2.0053 - val_acc: 0.5276\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 0.4149 - acc: 0.8523 - val_loss: 2.0331 - val_acc: 0.5279\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.3981 - acc: 0.8589 - val_loss: 2.0787 - val_acc: 0.5230\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.3718 - acc: 0.8692 - val_loss: 2.1667 - val_acc: 0.5236\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.3490 - acc: 0.8781 - val_loss: 2.0974 - val_acc: 0.5259\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 0.3349 - acc: 0.8816 - val_loss: 2.2658 - val_acc: 0.5232\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 0.3272 - acc: 0.8847 - val_loss: 2.3410 - val_acc: 0.5272\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.2952 - acc: 0.8958 - val_loss: 2.4256 - val_acc: 0.5196\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.2854 - acc: 0.8986 - val_loss: 2.5042 - val_acc: 0.5262\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 0.2638 - acc: 0.9084 - val_loss: 2.4040 - val_acc: 0.5279\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.2429 - acc: 0.9149 - val_loss: 2.4134 - val_acc: 0.5301\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 0.2316 - acc: 0.9186 - val_loss: 2.5425 - val_acc: 0.5190\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.2296 - acc: 0.9187 - val_loss: 2.7348 - val_acc: 0.5232\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 0.2157 - acc: 0.9244 - val_loss: 2.6630 - val_acc: 0.5239\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.1979 - acc: 0.9307 - val_loss: 2.6938 - val_acc: 0.5173\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 0.1954 - acc: 0.9328 - val_loss: 2.7788 - val_acc: 0.5241\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.1905 - acc: 0.9332 - val_loss: 2.8979 - val_acc: 0.5289\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.1801 - acc: 0.9374 - val_loss: 2.8854 - val_acc: 0.5247\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 0.1654 - acc: 0.9422 - val_loss: 3.0373 - val_acc: 0.5243\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 0.1722 - acc: 0.9396 - val_loss: 2.9624 - val_acc: 0.5305\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.1646 - acc: 0.9427 - val_loss: 2.8402 - val_acc: 0.5272\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.1478 - acc: 0.9493 - val_loss: 2.9909 - val_acc: 0.5311\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.1402 - acc: 0.9516 - val_loss: 3.0976 - val_acc: 0.5260\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 0.1406 - acc: 0.9509 - val_loss: 3.1313 - val_acc: 0.5298\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 0.1407 - acc: 0.9517 - val_loss: 3.0776 - val_acc: 0.5271\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 0.1322 - acc: 0.9537 - val_loss: 3.1836 - val_acc: 0.5288\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 0.1326 - acc: 0.9541 - val_loss: 3.2338 - val_acc: 0.5291\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 0.1283 - acc: 0.9572 - val_loss: 3.2304 - val_acc: 0.5291\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 0.1054 - acc: 0.9647 - val_loss: 3.2366 - val_acc: 0.5300\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 0.1072 - acc: 0.9641 - val_loss: 3.2344 - val_acc: 0.5325\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 0.0867 - acc: 0.9700 - val_loss: 3.3097 - val_acc: 0.5285\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 0.0848 - acc: 0.9704 - val_loss: 3.4836 - val_acc: 0.5305\n",
            "Experiment with LR = 0.001000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 2.2611 - acc: 0.1560 - val_loss: 2.2048 - val_acc: 0.2149\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.1473 - acc: 0.2466 - val_loss: 2.0832 - val_acc: 0.2799\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.0313 - acc: 0.2926 - val_loss: 1.9723 - val_acc: 0.3165\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 1.9294 - acc: 0.3282 - val_loss: 1.8817 - val_acc: 0.3418\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 1.8510 - acc: 0.3492 - val_loss: 1.8176 - val_acc: 0.3583\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 1.7930 - acc: 0.3676 - val_loss: 1.7685 - val_acc: 0.3798\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 1.7475 - acc: 0.3820 - val_loss: 1.7285 - val_acc: 0.3908\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 1.7093 - acc: 0.3955 - val_loss: 1.6949 - val_acc: 0.4012\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 1.6763 - acc: 0.4067 - val_loss: 1.6671 - val_acc: 0.4103\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 1.6476 - acc: 0.4165 - val_loss: 1.6430 - val_acc: 0.4182\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 1.6222 - acc: 0.4263 - val_loss: 1.6214 - val_acc: 0.4280\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 1.5985 - acc: 0.4331 - val_loss: 1.6055 - val_acc: 0.4326\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 1.5773 - acc: 0.4398 - val_loss: 1.5888 - val_acc: 0.4413\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 1.5569 - acc: 0.4491 - val_loss: 1.5713 - val_acc: 0.4442\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 1.5377 - acc: 0.4556 - val_loss: 1.5583 - val_acc: 0.4481\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 1.5187 - acc: 0.4621 - val_loss: 1.5437 - val_acc: 0.4560\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 1.5001 - acc: 0.4690 - val_loss: 1.5312 - val_acc: 0.4580\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 11s 229us/step - loss: 1.4826 - acc: 0.4764 - val_loss: 1.5219 - val_acc: 0.4618\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 1.4652 - acc: 0.4817 - val_loss: 1.5086 - val_acc: 0.4672\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 1.4482 - acc: 0.4888 - val_loss: 1.4979 - val_acc: 0.4706\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 12s 230us/step - loss: 1.4319 - acc: 0.4949 - val_loss: 1.4890 - val_acc: 0.4734\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 1.4158 - acc: 0.5015 - val_loss: 1.4787 - val_acc: 0.4767\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 1.4001 - acc: 0.5056 - val_loss: 1.4693 - val_acc: 0.4817\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 11s 229us/step - loss: 1.3848 - acc: 0.5110 - val_loss: 1.4612 - val_acc: 0.4839\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 1.3694 - acc: 0.5167 - val_loss: 1.4571 - val_acc: 0.4840\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 1.3546 - acc: 0.5237 - val_loss: 1.4480 - val_acc: 0.4871\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 1.3402 - acc: 0.5287 - val_loss: 1.4401 - val_acc: 0.4924\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 1.3254 - acc: 0.5334 - val_loss: 1.4323 - val_acc: 0.4948\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 1.3112 - acc: 0.5408 - val_loss: 1.4253 - val_acc: 0.4943\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 1.2972 - acc: 0.5453 - val_loss: 1.4214 - val_acc: 0.4961\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 1.2829 - acc: 0.5509 - val_loss: 1.4142 - val_acc: 0.5002\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 1.2696 - acc: 0.5550 - val_loss: 1.4128 - val_acc: 0.5026\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 1.2563 - acc: 0.5603 - val_loss: 1.4092 - val_acc: 0.4999\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 1.2426 - acc: 0.5645 - val_loss: 1.4055 - val_acc: 0.5027\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 1.2294 - acc: 0.5689 - val_loss: 1.4078 - val_acc: 0.5054\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 1.2164 - acc: 0.5725 - val_loss: 1.3961 - val_acc: 0.5104\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 1.2032 - acc: 0.5780 - val_loss: 1.3946 - val_acc: 0.5079\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.1904 - acc: 0.5818 - val_loss: 1.3933 - val_acc: 0.5097\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 1.1769 - acc: 0.5880 - val_loss: 1.3955 - val_acc: 0.5106\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 1.1642 - acc: 0.5923 - val_loss: 1.4026 - val_acc: 0.5104\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 1.1520 - acc: 0.5967 - val_loss: 1.3906 - val_acc: 0.5112\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 1.1393 - acc: 0.6002 - val_loss: 1.3877 - val_acc: 0.5127\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 1.1260 - acc: 0.6060 - val_loss: 1.4016 - val_acc: 0.5104\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 1.1133 - acc: 0.6096 - val_loss: 1.3949 - val_acc: 0.5121\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 1.1009 - acc: 0.6148 - val_loss: 1.3893 - val_acc: 0.5113\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 1.0883 - acc: 0.6207 - val_loss: 1.3824 - val_acc: 0.5180\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 1.0752 - acc: 0.6242 - val_loss: 1.3893 - val_acc: 0.5153\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 12s 231us/step - loss: 1.0632 - acc: 0.6282 - val_loss: 1.3902 - val_acc: 0.5153\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 1.0502 - acc: 0.6322 - val_loss: 1.3880 - val_acc: 0.5197\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 1.0378 - acc: 0.6373 - val_loss: 1.3999 - val_acc: 0.5159\n",
            "Experiment with LR = 0.001000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.2570 - acc: 0.1665 - val_loss: 2.2031 - val_acc: 0.2264\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.1445 - acc: 0.2559 - val_loss: 2.0860 - val_acc: 0.2754\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.0303 - acc: 0.2907 - val_loss: 1.9784 - val_acc: 0.3018\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 1.9335 - acc: 0.3199 - val_loss: 1.8953 - val_acc: 0.3315\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 1.8614 - acc: 0.3426 - val_loss: 1.8348 - val_acc: 0.3527\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 1.8066 - acc: 0.3613 - val_loss: 1.7875 - val_acc: 0.3682\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 1.7621 - acc: 0.3763 - val_loss: 1.7470 - val_acc: 0.3815\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 1.7244 - acc: 0.3886 - val_loss: 1.7138 - val_acc: 0.3928\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 1.6921 - acc: 0.4016 - val_loss: 1.6850 - val_acc: 0.4027\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 1.6642 - acc: 0.4110 - val_loss: 1.6608 - val_acc: 0.4109\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 1.6395 - acc: 0.4205 - val_loss: 1.6397 - val_acc: 0.4202\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 1.6174 - acc: 0.4266 - val_loss: 1.6215 - val_acc: 0.4282\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 1.5963 - acc: 0.4349 - val_loss: 1.6031 - val_acc: 0.4361\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 1.5767 - acc: 0.4421 - val_loss: 1.5889 - val_acc: 0.4381\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 1.5578 - acc: 0.4489 - val_loss: 1.5737 - val_acc: 0.4465\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 11s 222us/step - loss: 1.5396 - acc: 0.4550 - val_loss: 1.5596 - val_acc: 0.4509\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 1.5225 - acc: 0.4610 - val_loss: 1.5467 - val_acc: 0.4531\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 1.5049 - acc: 0.4685 - val_loss: 1.5327 - val_acc: 0.4584\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.4888 - acc: 0.4729 - val_loss: 1.5215 - val_acc: 0.4632\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 1.4721 - acc: 0.4796 - val_loss: 1.5134 - val_acc: 0.4641\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.4564 - acc: 0.4857 - val_loss: 1.4984 - val_acc: 0.4708\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 1.4405 - acc: 0.4914 - val_loss: 1.4904 - val_acc: 0.4759\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.4259 - acc: 0.4967 - val_loss: 1.4790 - val_acc: 0.4807\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.4106 - acc: 0.5034 - val_loss: 1.4711 - val_acc: 0.4820\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.3962 - acc: 0.5094 - val_loss: 1.4615 - val_acc: 0.4857\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 1.3824 - acc: 0.5137 - val_loss: 1.4519 - val_acc: 0.4869\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.3679 - acc: 0.5180 - val_loss: 1.4470 - val_acc: 0.4900\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 1.3536 - acc: 0.5242 - val_loss: 1.4395 - val_acc: 0.4908\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 1.3402 - acc: 0.5297 - val_loss: 1.4348 - val_acc: 0.4938\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.3264 - acc: 0.5342 - val_loss: 1.4325 - val_acc: 0.4940\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 1.3132 - acc: 0.5375 - val_loss: 1.4200 - val_acc: 0.5010\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.2995 - acc: 0.5425 - val_loss: 1.4188 - val_acc: 0.5029\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.2874 - acc: 0.5488 - val_loss: 1.4092 - val_acc: 0.5042\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.2741 - acc: 0.5534 - val_loss: 1.4062 - val_acc: 0.5067\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.2617 - acc: 0.5570 - val_loss: 1.4050 - val_acc: 0.5048\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 1.2485 - acc: 0.5621 - val_loss: 1.4055 - val_acc: 0.5027\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.2365 - acc: 0.5655 - val_loss: 1.3956 - val_acc: 0.5119\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.2242 - acc: 0.5702 - val_loss: 1.3915 - val_acc: 0.5114\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 1.2113 - acc: 0.5745 - val_loss: 1.3909 - val_acc: 0.5120\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.1995 - acc: 0.5792 - val_loss: 1.3856 - val_acc: 0.5137\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 1.1869 - acc: 0.5843 - val_loss: 1.3817 - val_acc: 0.5167\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 1.1740 - acc: 0.5878 - val_loss: 1.3806 - val_acc: 0.5162\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.1628 - acc: 0.5920 - val_loss: 1.3793 - val_acc: 0.5157\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 1.1514 - acc: 0.5950 - val_loss: 1.3848 - val_acc: 0.5142\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 1.1385 - acc: 0.6007 - val_loss: 1.3783 - val_acc: 0.5170\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 1.1261 - acc: 0.6047 - val_loss: 1.3750 - val_acc: 0.5210\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 1.1148 - acc: 0.6087 - val_loss: 1.3835 - val_acc: 0.5109\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 1.1020 - acc: 0.6124 - val_loss: 1.3792 - val_acc: 0.5185\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 1.0908 - acc: 0.6150 - val_loss: 1.3794 - val_acc: 0.5172\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 1.0776 - acc: 0.6212 - val_loss: 1.3761 - val_acc: 0.5228\n",
            "Experiment with LR = 0.000100\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 2.3059 - acc: 0.0966 - val_loss: 2.3047 - val_acc: 0.0964\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.3016 - acc: 0.1050 - val_loss: 2.3004 - val_acc: 0.1063\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.2975 - acc: 0.1150 - val_loss: 2.2964 - val_acc: 0.1160\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.2935 - acc: 0.1252 - val_loss: 2.2925 - val_acc: 0.1281\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.2897 - acc: 0.1348 - val_loss: 2.2886 - val_acc: 0.1379\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.2859 - acc: 0.1435 - val_loss: 2.2848 - val_acc: 0.1453\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.2822 - acc: 0.1517 - val_loss: 2.2810 - val_acc: 0.1520\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.2785 - acc: 0.1591 - val_loss: 2.2773 - val_acc: 0.1594\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.2748 - acc: 0.1653 - val_loss: 2.2735 - val_acc: 0.1656\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.2711 - acc: 0.1713 - val_loss: 2.2697 - val_acc: 0.1714\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.2673 - acc: 0.1772 - val_loss: 2.2658 - val_acc: 0.1783\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.2635 - acc: 0.1829 - val_loss: 2.2619 - val_acc: 0.1852\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.2596 - acc: 0.1866 - val_loss: 2.2579 - val_acc: 0.1913\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.2556 - acc: 0.1900 - val_loss: 2.2538 - val_acc: 0.1951\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.2515 - acc: 0.1939 - val_loss: 2.2496 - val_acc: 0.1983\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.2474 - acc: 0.1964 - val_loss: 2.2453 - val_acc: 0.2002\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 2.2431 - acc: 0.2000 - val_loss: 2.2409 - val_acc: 0.2040\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 2.2388 - acc: 0.2022 - val_loss: 2.2364 - val_acc: 0.2075\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 2.2344 - acc: 0.2052 - val_loss: 2.2318 - val_acc: 0.2101\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.2298 - acc: 0.2082 - val_loss: 2.2271 - val_acc: 0.2132\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 2.2252 - acc: 0.2109 - val_loss: 2.2224 - val_acc: 0.2175\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 11s 229us/step - loss: 2.2205 - acc: 0.2132 - val_loss: 2.2175 - val_acc: 0.2218\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 11s 229us/step - loss: 2.2157 - acc: 0.2153 - val_loss: 2.2126 - val_acc: 0.2237\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.2108 - acc: 0.2184 - val_loss: 2.2076 - val_acc: 0.2255\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.2058 - acc: 0.2207 - val_loss: 2.2025 - val_acc: 0.2278\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.2008 - acc: 0.2227 - val_loss: 2.1974 - val_acc: 0.2293\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.1957 - acc: 0.2255 - val_loss: 2.1922 - val_acc: 0.2320\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.1905 - acc: 0.2283 - val_loss: 2.1870 - val_acc: 0.2354\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.1853 - acc: 0.2309 - val_loss: 2.1818 - val_acc: 0.2380\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.1801 - acc: 0.2336 - val_loss: 2.1765 - val_acc: 0.2409\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.1749 - acc: 0.2366 - val_loss: 2.1712 - val_acc: 0.2436\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.1696 - acc: 0.2394 - val_loss: 2.1659 - val_acc: 0.2448\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.1643 - acc: 0.2423 - val_loss: 2.1606 - val_acc: 0.2479\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.1590 - acc: 0.2450 - val_loss: 2.1552 - val_acc: 0.2514\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.1537 - acc: 0.2473 - val_loss: 2.1499 - val_acc: 0.2537\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.1484 - acc: 0.2501 - val_loss: 2.1445 - val_acc: 0.2562\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.1431 - acc: 0.2531 - val_loss: 2.1392 - val_acc: 0.2573\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.1378 - acc: 0.2552 - val_loss: 2.1339 - val_acc: 0.2594\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.1326 - acc: 0.2573 - val_loss: 2.1286 - val_acc: 0.2620\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.1273 - acc: 0.2598 - val_loss: 2.1234 - val_acc: 0.2651\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.1221 - acc: 0.2620 - val_loss: 2.1181 - val_acc: 0.2671\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.1170 - acc: 0.2646 - val_loss: 2.1129 - val_acc: 0.2694\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.1118 - acc: 0.2664 - val_loss: 2.1077 - val_acc: 0.2714\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.1067 - acc: 0.2681 - val_loss: 2.1025 - val_acc: 0.2730\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.1016 - acc: 0.2706 - val_loss: 2.0973 - val_acc: 0.2738\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.0965 - acc: 0.2730 - val_loss: 2.0921 - val_acc: 0.2764\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.0914 - acc: 0.2755 - val_loss: 2.0870 - val_acc: 0.2782\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.0864 - acc: 0.2771 - val_loss: 2.0818 - val_acc: 0.2788\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.0814 - acc: 0.2793 - val_loss: 2.0767 - val_acc: 0.2803\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 2.0764 - acc: 0.2810 - val_loss: 2.0717 - val_acc: 0.2833\n",
            "Experiment with LR = 0.000100\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 2.3186 - acc: 0.1075 - val_loss: 2.3127 - val_acc: 0.1136\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.3061 - acc: 0.1165 - val_loss: 2.3007 - val_acc: 0.1221\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.2951 - acc: 0.1273 - val_loss: 2.2900 - val_acc: 0.1331\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.2852 - acc: 0.1379 - val_loss: 2.2803 - val_acc: 0.1446\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.2762 - acc: 0.1503 - val_loss: 2.2714 - val_acc: 0.1579\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 2.2678 - acc: 0.1605 - val_loss: 2.2631 - val_acc: 0.1690\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 2.2599 - acc: 0.1715 - val_loss: 2.2551 - val_acc: 0.1772\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.2523 - acc: 0.1799 - val_loss: 2.2475 - val_acc: 0.1854\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 2.2450 - acc: 0.1875 - val_loss: 2.2401 - val_acc: 0.1941\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.2378 - acc: 0.1944 - val_loss: 2.2328 - val_acc: 0.2001\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.2308 - acc: 0.1998 - val_loss: 2.2256 - val_acc: 0.2049\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.2240 - acc: 0.2042 - val_loss: 2.2186 - val_acc: 0.2110\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.2172 - acc: 0.2087 - val_loss: 2.2117 - val_acc: 0.2157\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.2105 - acc: 0.2125 - val_loss: 2.2048 - val_acc: 0.2198\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.2038 - acc: 0.2160 - val_loss: 2.1981 - val_acc: 0.2214\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.1973 - acc: 0.2197 - val_loss: 2.1914 - val_acc: 0.2225\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.1908 - acc: 0.2220 - val_loss: 2.1847 - val_acc: 0.2273\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.1843 - acc: 0.2244 - val_loss: 2.1781 - val_acc: 0.2322\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.1779 - acc: 0.2274 - val_loss: 2.1716 - val_acc: 0.2352\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 11s 222us/step - loss: 2.1715 - acc: 0.2294 - val_loss: 2.1651 - val_acc: 0.2373\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.1652 - acc: 0.2312 - val_loss: 2.1587 - val_acc: 0.2391\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.1589 - acc: 0.2332 - val_loss: 2.1523 - val_acc: 0.2394\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.1527 - acc: 0.2351 - val_loss: 2.1459 - val_acc: 0.2426\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.1465 - acc: 0.2370 - val_loss: 2.1396 - val_acc: 0.2443\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.1404 - acc: 0.2388 - val_loss: 2.1334 - val_acc: 0.2478\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 2.1343 - acc: 0.2412 - val_loss: 2.1272 - val_acc: 0.2492\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 2.1283 - acc: 0.2439 - val_loss: 2.1210 - val_acc: 0.2510\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.1222 - acc: 0.2463 - val_loss: 2.1149 - val_acc: 0.2527\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.1162 - acc: 0.2489 - val_loss: 2.1088 - val_acc: 0.2559\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 2.1103 - acc: 0.2512 - val_loss: 2.1028 - val_acc: 0.2578\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 2.1044 - acc: 0.2526 - val_loss: 2.0968 - val_acc: 0.2605\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.0985 - acc: 0.2554 - val_loss: 2.0909 - val_acc: 0.2636\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 2.0927 - acc: 0.2580 - val_loss: 2.0850 - val_acc: 0.2656\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 2.0869 - acc: 0.2611 - val_loss: 2.0791 - val_acc: 0.2684\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 2.0812 - acc: 0.2632 - val_loss: 2.0733 - val_acc: 0.2711\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.0755 - acc: 0.2655 - val_loss: 2.0675 - val_acc: 0.2742\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.0698 - acc: 0.2677 - val_loss: 2.0618 - val_acc: 0.2762\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.0642 - acc: 0.2697 - val_loss: 2.0561 - val_acc: 0.2775\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.0586 - acc: 0.2716 - val_loss: 2.0505 - val_acc: 0.2797\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.0531 - acc: 0.2735 - val_loss: 2.0449 - val_acc: 0.2818\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.0476 - acc: 0.2756 - val_loss: 2.0393 - val_acc: 0.2845\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 2.0421 - acc: 0.2783 - val_loss: 2.0338 - val_acc: 0.2868\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.0366 - acc: 0.2804 - val_loss: 2.0284 - val_acc: 0.2894\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.0312 - acc: 0.2826 - val_loss: 2.0229 - val_acc: 0.2919\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.0258 - acc: 0.2849 - val_loss: 2.0175 - val_acc: 0.2941\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.0205 - acc: 0.2869 - val_loss: 2.0122 - val_acc: 0.2954\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.0152 - acc: 0.2890 - val_loss: 2.0069 - val_acc: 0.2980\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.0100 - acc: 0.2906 - val_loss: 2.0017 - val_acc: 0.3007\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.0048 - acc: 0.2929 - val_loss: 1.9965 - val_acc: 0.3023\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 1.9996 - acc: 0.2945 - val_loss: 1.9914 - val_acc: 0.3033\n",
            "Experiment with LR = 0.000010\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 12s 232us/step - loss: 2.3328 - acc: 0.0933 - val_loss: 2.3314 - val_acc: 0.0952\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3321 - acc: 0.0936 - val_loss: 2.3307 - val_acc: 0.0951\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3314 - acc: 0.0941 - val_loss: 2.3300 - val_acc: 0.0960\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3307 - acc: 0.0944 - val_loss: 2.3293 - val_acc: 0.0964\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.3300 - acc: 0.0948 - val_loss: 2.3287 - val_acc: 0.0966\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.3294 - acc: 0.0954 - val_loss: 2.3280 - val_acc: 0.0971\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 2.3287 - acc: 0.0959 - val_loss: 2.3273 - val_acc: 0.0971\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.3281 - acc: 0.0962 - val_loss: 2.3266 - val_acc: 0.0969\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.3274 - acc: 0.0966 - val_loss: 2.3260 - val_acc: 0.0973\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.3268 - acc: 0.0969 - val_loss: 2.3253 - val_acc: 0.0976\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3261 - acc: 0.0974 - val_loss: 2.3247 - val_acc: 0.0978\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3255 - acc: 0.0980 - val_loss: 2.3240 - val_acc: 0.0988\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3249 - acc: 0.0984 - val_loss: 2.3234 - val_acc: 0.0988\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.3243 - acc: 0.0989 - val_loss: 2.3228 - val_acc: 0.0988\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 12s 230us/step - loss: 2.3237 - acc: 0.0991 - val_loss: 2.3221 - val_acc: 0.0994\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.3230 - acc: 0.0996 - val_loss: 2.3215 - val_acc: 0.0999\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.3224 - acc: 0.1001 - val_loss: 2.3209 - val_acc: 0.1002\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3218 - acc: 0.1005 - val_loss: 2.3203 - val_acc: 0.1010\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 11s 229us/step - loss: 2.3213 - acc: 0.1010 - val_loss: 2.3197 - val_acc: 0.1014\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 2.3207 - acc: 0.1016 - val_loss: 2.3191 - val_acc: 0.1020\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 2.3201 - acc: 0.1020 - val_loss: 2.3185 - val_acc: 0.1015\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 2.3195 - acc: 0.1024 - val_loss: 2.3179 - val_acc: 0.1020\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.3189 - acc: 0.1027 - val_loss: 2.3173 - val_acc: 0.1025\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.3184 - acc: 0.1032 - val_loss: 2.3167 - val_acc: 0.1030\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.3178 - acc: 0.1038 - val_loss: 2.3161 - val_acc: 0.1033\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3172 - acc: 0.1044 - val_loss: 2.3155 - val_acc: 0.1042\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3167 - acc: 0.1050 - val_loss: 2.3150 - val_acc: 0.1044\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3161 - acc: 0.1054 - val_loss: 2.3144 - val_acc: 0.1050\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 2.3156 - acc: 0.1055 - val_loss: 2.3138 - val_acc: 0.1057\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3150 - acc: 0.1060 - val_loss: 2.3132 - val_acc: 0.1066\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3145 - acc: 0.1065 - val_loss: 2.3127 - val_acc: 0.1074\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3139 - acc: 0.1068 - val_loss: 2.3121 - val_acc: 0.1079\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3134 - acc: 0.1072 - val_loss: 2.3116 - val_acc: 0.1086\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.3129 - acc: 0.1078 - val_loss: 2.3110 - val_acc: 0.1089\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.3124 - acc: 0.1084 - val_loss: 2.3105 - val_acc: 0.1095\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 11s 223us/step - loss: 2.3118 - acc: 0.1088 - val_loss: 2.3099 - val_acc: 0.1099\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.3113 - acc: 0.1091 - val_loss: 2.3094 - val_acc: 0.1105\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.3108 - acc: 0.1097 - val_loss: 2.3089 - val_acc: 0.1108\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3103 - acc: 0.1101 - val_loss: 2.3083 - val_acc: 0.1113\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.3098 - acc: 0.1107 - val_loss: 2.3078 - val_acc: 0.1120\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.3093 - acc: 0.1114 - val_loss: 2.3073 - val_acc: 0.1129\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 11s 228us/step - loss: 2.3088 - acc: 0.1118 - val_loss: 2.3068 - val_acc: 0.1132\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.3083 - acc: 0.1124 - val_loss: 2.3063 - val_acc: 0.1141\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.3078 - acc: 0.1130 - val_loss: 2.3057 - val_acc: 0.1144\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3073 - acc: 0.1137 - val_loss: 2.3052 - val_acc: 0.1148\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.3068 - acc: 0.1140 - val_loss: 2.3047 - val_acc: 0.1160\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.3063 - acc: 0.1145 - val_loss: 2.3042 - val_acc: 0.1168\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.3058 - acc: 0.1150 - val_loss: 2.3037 - val_acc: 0.1174\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 11s 226us/step - loss: 2.3053 - acc: 0.1158 - val_loss: 2.3032 - val_acc: 0.1180\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 11s 227us/step - loss: 2.3048 - acc: 0.1163 - val_loss: 2.3027 - val_acc: 0.1182\n",
            "Experiment with LR = 0.000010\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,771,914\n",
            "Trainable params: 1,771,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.3129 - acc: 0.1062 - val_loss: 2.3141 - val_acc: 0.1060\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.3125 - acc: 0.1066 - val_loss: 2.3137 - val_acc: 0.1068\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.3121 - acc: 0.1071 - val_loss: 2.3133 - val_acc: 0.1068\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.3116 - acc: 0.1076 - val_loss: 2.3129 - val_acc: 0.1067\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.3112 - acc: 0.1080 - val_loss: 2.3125 - val_acc: 0.1070\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.3108 - acc: 0.1086 - val_loss: 2.3121 - val_acc: 0.1078\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 2.3104 - acc: 0.1091 - val_loss: 2.3117 - val_acc: 0.1081\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 2.3100 - acc: 0.1096 - val_loss: 2.3113 - val_acc: 0.1085\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 2.3096 - acc: 0.1103 - val_loss: 2.3109 - val_acc: 0.1087\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.3092 - acc: 0.1106 - val_loss: 2.3105 - val_acc: 0.1094\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 2.3088 - acc: 0.1111 - val_loss: 2.3101 - val_acc: 0.1102\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.3085 - acc: 0.1115 - val_loss: 2.3097 - val_acc: 0.1112\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.3081 - acc: 0.1118 - val_loss: 2.3093 - val_acc: 0.1114\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.3077 - acc: 0.1121 - val_loss: 2.3089 - val_acc: 0.1118\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.3073 - acc: 0.1127 - val_loss: 2.3085 - val_acc: 0.1120\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.3069 - acc: 0.1132 - val_loss: 2.3081 - val_acc: 0.1124\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.3065 - acc: 0.1137 - val_loss: 2.3077 - val_acc: 0.1128\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.3062 - acc: 0.1140 - val_loss: 2.3074 - val_acc: 0.1132\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.3058 - acc: 0.1146 - val_loss: 2.3070 - val_acc: 0.1134\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 11s 222us/step - loss: 2.3054 - acc: 0.1150 - val_loss: 2.3066 - val_acc: 0.1137\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.3050 - acc: 0.1157 - val_loss: 2.3062 - val_acc: 0.1149\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 2.3047 - acc: 0.1162 - val_loss: 2.3058 - val_acc: 0.1157\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 2.3043 - acc: 0.1165 - val_loss: 2.3055 - val_acc: 0.1163\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 11s 222us/step - loss: 2.3039 - acc: 0.1169 - val_loss: 2.3051 - val_acc: 0.1166\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.3036 - acc: 0.1173 - val_loss: 2.3047 - val_acc: 0.1169\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 11s 224us/step - loss: 2.3032 - acc: 0.1174 - val_loss: 2.3044 - val_acc: 0.1173\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 11s 222us/step - loss: 2.3029 - acc: 0.1181 - val_loss: 2.3040 - val_acc: 0.1175\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.3025 - acc: 0.1184 - val_loss: 2.3036 - val_acc: 0.1177\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.3021 - acc: 0.1190 - val_loss: 2.3033 - val_acc: 0.1181\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.3018 - acc: 0.1193 - val_loss: 2.3029 - val_acc: 0.1187\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.3014 - acc: 0.1199 - val_loss: 2.3026 - val_acc: 0.1193\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.3011 - acc: 0.1204 - val_loss: 2.3022 - val_acc: 0.1196\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.3007 - acc: 0.1209 - val_loss: 2.3018 - val_acc: 0.1199\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.3004 - acc: 0.1215 - val_loss: 2.3015 - val_acc: 0.1206\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.3000 - acc: 0.1217 - val_loss: 2.3011 - val_acc: 0.1210\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 2.2997 - acc: 0.1223 - val_loss: 2.3008 - val_acc: 0.1219\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.2993 - acc: 0.1227 - val_loss: 2.3004 - val_acc: 0.1223\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.2990 - acc: 0.1232 - val_loss: 2.3001 - val_acc: 0.1226\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.2986 - acc: 0.1237 - val_loss: 2.2997 - val_acc: 0.1227\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.2983 - acc: 0.1242 - val_loss: 2.2994 - val_acc: 0.1232\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 2.2980 - acc: 0.1250 - val_loss: 2.2991 - val_acc: 0.1238\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.2976 - acc: 0.1256 - val_loss: 2.2987 - val_acc: 0.1240\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.2973 - acc: 0.1259 - val_loss: 2.2984 - val_acc: 0.1247\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.2969 - acc: 0.1264 - val_loss: 2.2980 - val_acc: 0.1252\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 2.2966 - acc: 0.1268 - val_loss: 2.2977 - val_acc: 0.1253\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 11s 218us/step - loss: 2.2963 - acc: 0.1274 - val_loss: 2.2974 - val_acc: 0.1259\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 11s 221us/step - loss: 2.2959 - acc: 0.1279 - val_loss: 2.2970 - val_acc: 0.1268\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 11s 225us/step - loss: 2.2956 - acc: 0.1285 - val_loss: 2.2967 - val_acc: 0.1271\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 11s 220us/step - loss: 2.2953 - acc: 0.1290 - val_loss: 2.2964 - val_acc: 0.1276\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 11s 219us/step - loss: 2.2949 - acc: 0.1295 - val_loss: 2.2960 - val_acc: 0.1276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLsN_zANIn8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "outputId": "9b81e41d-87d1-4398-86cc-4b664e5dd951"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\"\"\"Code Here\n",
        "將結果繪出\n",
        "\"\"\"\n",
        "color_bar = [\"r\", \"g\", \"b\", \"y\", \"m\", \"k\",\"o\",\"w\",\"p\",\"s\"]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "for i, cond in enumerate(results.keys()):\n",
        "    plt.plot(range(len(results[cond]['train-loss'])),results[cond]['train-loss'], '-', label=cond, color=color_bar[i])\n",
        "    plt.plot(range(len(results[cond]['valid-loss'])),results[cond]['valid-loss'], '--', label=cond, color=color_bar[i])\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "for i, cond in enumerate(results.keys()):\n",
        "    plt.plot(range(len(results[cond]['train-acc'])),results[cond]['train-acc'], '-', label=cond, color=color_bar[i])\n",
        "    plt.plot(range(len(results[cond]['valid-acc'])),results[cond]['valid-acc'], '--', label=cond, color=color_bar[i])\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAF1CAYAAAA5lJkfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZAwoIIuIiIyB/wckfpixmS7qIAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAF1CAYAAADSoyIcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDgLIYQQw8z/B7FTQ1N0i5WxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}